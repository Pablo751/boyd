{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5238f196-c670-47e0-a0cb-ddb3613c54bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main function\n",
      "Reading CSV file at path: /Users/juanpablocasadobissone/Downloads/Copy of SEO Team Content Requirements - Sheet58.csv\n",
      "Total rows in CSV: 105\n",
      "WebDriver initialized successfully.\n",
      "Keyword 'christmas party venues in dublin' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'cahonas boxer shorts' already exists in sheet 'Product Sheet'. Skipping...\n",
      "Keyword 'water meter audit' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'business water hygiene risk assessment' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'business water testing' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'testing water for legionella' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'business water sampling' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'quotes to save business water' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'average business water usage' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'business water usage calculator' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'business water usage per day' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'business water bill estimate' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'General waste management services' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'secure shredding' case study. Target following varients: 'confidential shredding glasgow' - 'confidential shredding edinburgh'' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'buy bird food online' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'henry bell bird food' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'maxi cosi rodifix pro 2 i size car seat' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'maxi cosi zelia 3 pushchair' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'nipper single v5 pushchair' already exists in sheet 'Product Sheet'. Skipping...\n",
      "Keyword 'recaro salia 125 i size car seat' already exists in sheet 'Product Sheet'. Skipping...\n",
      "Keyword 'thule urban glide 2 jogging stroller' already exists in sheet 'Product Sheet'. Skipping...\n",
      "Keyword 'wedding planning ayrshire' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'how to plan a wedding in scotland' already exists in sheet 'Blog Sheet'. Skipping...\n",
      "Keyword 'reverse osmosis membrane housings' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'reverse osmosis membrane housing kits' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'reverse osmosis fittings' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'reverse osmosis flow restrictors' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'reverse osmosis flush valve assemblies' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'reverse osmosis spares' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'reverse osmosis pressure gauges' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'reverse osmosis tanks' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'reverse osmosis tds meters' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'reverse osmosis tubing' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword '3 stage aquarium reverse osmosis replacement filters' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword '4 stage aquarium reverse osmosis replacement filters' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword '4 stage domestic reverse osmosis replacement filters' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword '5 stage domestic reverse osmosis replacement filters' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword '6 stage domestic reverse osmosis replacement filters' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'reverse osmosis drinking water replacement filters' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'used reception furniture' already exists in sheet 'Blog Sheet'. Skipping...\n",
      "Keyword 'used straight desks' already exists in sheet 'Blog Sheet'. Skipping...\n",
      "Keyword 'agricultural fence posts' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'chainlink fencing' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'concrete posts' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'fence boards' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'fence panels' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'fence post boltdowns' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'fence post caps' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'fence rails' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'garden gates' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'gate fixings' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'post concrete' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'temporary fencing' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'timber joinery' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'other hardwoods' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'oak hardwood' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'sapele hardwood' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'timber cladding' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'cbd oil for dogs' already exists in sheet 'Blog Sheet'. Skipping...\n",
      "Keyword 'wholesale christmas candy balloons OR wholesale christmas gingerbread balloons and variations' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'wholesale valentine's day balloons' already exists in sheet 'Category Sheet'. Skipping...\n",
      "Keyword 'wholesale northstar balloons' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'wholesale sempertex balloons' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'wholesale balloon training' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'How to prevent frozen pipes' already exists in sheet 'Blog Sheet'. Skipping...\n",
      "Keyword '6mm mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '9mm mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '12mm mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '18mm mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '19mm mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'standard grade mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '6mm standard grade mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '9mm standard grade mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '12mm standard grade mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '18mm standard grade mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '19mm standard grade mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'moisture resistant mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '6mm moisture resistant mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '9mm moisture resistant mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '12mm moisture resistant mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '18mm moisture resistant mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword '19mm moisture resistant mdf' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'How to waterproof plywood' already exists in sheet 'Blog Sheet'. Skipping...\n",
      "Keyword 'legal client management software' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'legal lead generation software' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'legal client onboarding' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'legal workflow management software' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'conveyancing quote software' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'solar tiles' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'commercial solar panels' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'car restraint systems' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'car roof box hoist' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in anniesland' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in broomhill' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in glasgow city centre' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in dowanhill' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in finneston' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in hillhead' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in hyndland' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in jordanhill' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property merchant city' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in park circus' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in partick' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in thornwood' already exists in sheet 'Landing Page Sheet'. Skipping...\n",
      "Keyword 'let your property in glasgow woodlands' already exists in sheet 'Landing Page Sheet'. Skipping...\n"
     ]
    }
   ],
   "source": [
    "#Brief script\n",
    "\n",
    "import openai\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time \n",
    "\n",
    "scopes = [\n",
    "    'https://www.googleapis.com/auth/spreadsheets',\n",
    "    'https://www.googleapis.com/auth/documents',\n",
    "    'https://www.googleapis.com/auth/drive'\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = '/Users/juanpablocasadobissone/Downloads/Copy of SEO Team Content Requirements - Sheet58.csv'\n",
    "\n",
    "# Initialize the credentials with the defined scopes\n",
    "credentials = Credentials.from_service_account_file('/Users/juanpablocasadobissone/Downloads/boyd-digital-scripts-50288f948562.json', scopes=scopes)\n",
    "\n",
    "# Initialize the Drive API client\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# Initialize the Sheets API client\n",
    "sheets_service = build('sheets', 'v4', credentials=credentials)\n",
    "spreadsheet_id = '1U0hDg29g4Uh45LgX7tZD8SHUOA-6m9G-eaCSJf9RBS4'\n",
    "\n",
    "# Initialize the Docs API client\n",
    "docs_service = build('docs', 'v1', credentials=credentials)\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def read_data_from_sheet(sheet_name):\n",
    "    range_name = f\"{sheet_name}!A:A\"  # Assuming the keyword is in the first column\n",
    "    try:\n",
    "        result = sheets_service.spreadsheets().values().get(\n",
    "            spreadsheetId=spreadsheet_id, range=range_name).execute()\n",
    "        values = result.get('values', [])\n",
    "        return [row[0] for row in values if row]  # Extract keywords from each row\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading from sheet {sheet_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "def read_keywords_and_content_types(csv_file_path):\n",
    "    print(f\"Reading CSV file at path: {csv_file_path}\")\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        rows = list(reader)  # Store all rows\n",
    "\n",
    "    print(f\"Total rows in CSV: {len(rows)}\")\n",
    "    \n",
    "    keywords = [row['Core Target Keyword'].strip() for row in rows]\n",
    "    content_types = {row['Core Target Keyword'].strip(): row['Content Type'].strip().lower() for row in rows}\n",
    "    \n",
    "    #print(\"Keywords:\", keywords)\n",
    "    #print(\"Content Types:\", content_types)\n",
    "    \n",
    "    return keywords, content_types, rows\n",
    "\n",
    "def update_csv_with_brief(csv_file_path, rows, doc_links):\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = list(rows[0].keys()) + ['Brief']  # Add 'Brief' to the column headers\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in rows:\n",
    "            keyword = row['Core Target Keyword']\n",
    "            brief = doc_links.get(keyword, \"\")\n",
    "            row['Generated Brief'] = brief\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def determine_sheet_name(content_type):\n",
    "    content_type = content_type.lower()\n",
    "    if \"blog\" in content_type:\n",
    "        return \"Blog Sheet\"\n",
    "    elif \"branch\" in content_type:\n",
    "        return \"Branch Sheet\"\n",
    "    elif \"category\" in content_type:\n",
    "        return \"Category Sheet\"\n",
    "    elif \"landing\" in content_type:\n",
    "        return \"Landing Page Sheet\"\n",
    "    elif \"pr article\" in content_type:\n",
    "        return \"PR Article Sheet\"\n",
    "    elif \"intent page\" in content_type:\n",
    "        return \"Intent Page Sheet\"\n",
    "    elif \"product\" in content_type:\n",
    "        return \"Product Sheet\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid content type: {content_type}\")\n",
    "\n",
    "\n",
    "\n",
    "# Define the function to find related queries\n",
    "def find_related_queries(driver, query):\n",
    "    global first_execution\n",
    "    query = query.strip()\n",
    "    related_queries = \"\"\n",
    "\n",
    "    try:\n",
    "        search_url = f\"https://www.google.com/search?q={query}\"\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # Wait for the page to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//div[@data-hveid]\"))\n",
    "        )\n",
    "\n",
    "        # Get the HTML content of the page\n",
    "        html_content = driver.page_source\n",
    "\n",
    "        # Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find the parent div containing all the related search terms\n",
    "        parent_div = soup.find('div', class_='y6Uyqe') or soup.find('div', class_='Wt5Tfe') or soup.find('div', class_='EIaa9b')\n",
    "\n",
    "        # Check if parent_div is not None\n",
    "        if parent_div is not None:\n",
    "            div_elements = parent_div.find_all('div', attrs={'data-hveid': True})\n",
    "            found_texts = set()  # Using a set to avoid duplicates\n",
    "\n",
    "            for div in div_elements:\n",
    "                term_div = div.find('div', class_='s75CSd u60jwe r2fjmd AB4Wff')\n",
    "                if term_div:\n",
    "                    found_texts.add(term_div.text)\n",
    "\n",
    "            # Join the found texts with commas\n",
    "            related_queries = ', '.join(found_texts)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to find elements: {e}\")\n",
    "    finally:\n",
    "        # Close the current browser tab even if an exception occurs\n",
    "        #driver.close()\n",
    "        print(f\"Found texts: {related_queries}\")\n",
    "\n",
    "    return related_queries\n",
    "\n",
    "\n",
    "\n",
    "def generate_section(keyword, section_name, prompt):\n",
    "    full_prompt = f\"{prompt} Please limit the {section_name} to 100 tokens.\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in generating content briefs for digital marketing.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"In maximum 100 tokens, generate the {section_name} for the keyword '{keyword}' with the following prompt: {full_prompt}\"}\n",
    "        ],\n",
    "    )\n",
    "    content = response['choices'][0]['message']['content']\n",
    "    return content\n",
    "\n",
    "# Function to generate FAQ questions\n",
    "def generate_faq_questions(keyword, prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in generating content briefs for digital marketing.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Generate three FAQ questions for the keyword '{keyword}' with the following prompt: {prompt}. Please keep the questions brief.\"}\n",
    "        ],\n",
    "    )\n",
    "    content = response['choices'][0]['message']['content']\n",
    "    return content.split('\\n')[:3]  # Ensure only three questions are returned\n",
    "\n",
    "# Function to generate FAQ answers based on questions\n",
    "def generate_faq_answers(keyword, questions, prompt):\n",
    "    questions_str = '\\n'.join(questions)\n",
    "    full_prompt = f\"{prompt}\\nI will provide the questions. Please just give the answers, for each one assign it a number.\\n{questions_str}\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in generating content briefs for digital marketing.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Generate answers for these FAQ questions about '{keyword}' with the following prompt: {full_prompt}.\"}\n",
    "        ],\n",
    "    )\n",
    "    content = response['choices'][0]['message']['content']\n",
    "    # Split and return only as many answers as there are questions\n",
    "    lines = [line.strip() for line in content.split('\\n') if line.strip() and line.strip()[0].isdigit()]\n",
    "    return lines[:len(questions)]\n",
    "\n",
    "\n",
    "def write_to_sheet(sheet_name, headers, data):\n",
    "    # Check if the sheet exists, if not create it\n",
    "    sheet_metadata = sheets_service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()\n",
    "    sheets = sheet_metadata.get('sheets', '')\n",
    "    sheet_exists = any(sheet['properties']['title'] == sheet_name for sheet in sheets)\n",
    "    \n",
    "    if not sheet_exists:\n",
    "        body = {\n",
    "            'requests': [\n",
    "                {'addSheet': {'properties': {'title': sheet_name}}}\n",
    "            ]\n",
    "        }\n",
    "        sheets_service.spreadsheets().batchUpdate(spreadsheetId=spreadsheet_id, body=body).execute()\n",
    "    \n",
    "    # Define the header range based on the length of the headers list\n",
    "    header_range = f\"{sheet_name}!A1:{chr(64 + len(headers))}1\"\n",
    "    \n",
    "    # Check if headers exist and write them if necessary\n",
    "    header_data = sheets_service.spreadsheets().values().get(spreadsheetId=spreadsheet_id, range=header_range).execute().get('values', [])\n",
    "    \n",
    "    # Rewrite headers if they are wrong or in the wrong position\n",
    "    if header_data != [headers]:\n",
    "        sheets_service.spreadsheets().values().update(\n",
    "            spreadsheetId=spreadsheet_id,\n",
    "            range=header_range,\n",
    "            body={\"values\": [headers]},\n",
    "            valueInputOption=\"RAW\"\n",
    "        ).execute()\n",
    "    \n",
    "    # Write data to the sheet\n",
    "    body = {\n",
    "        'values': data\n",
    "    }\n",
    "    sheets_service.spreadsheets().values().append(\n",
    "        spreadsheetId=spreadsheet_id,\n",
    "        range=f\"{sheet_name}!A1\",\n",
    "        body=body,\n",
    "        valueInputOption=\"RAW\"\n",
    "    ).execute()\n",
    "    \n",
    "def create_google_doc(title):\n",
    "    body = {\n",
    "        'title': title\n",
    "    }\n",
    "    doc = docs_service.documents().create(body=body).execute()\n",
    "    return doc['documentId']\n",
    "\n",
    "def add_editor_to_doc(file_id, email):\n",
    "    user_permission = {\n",
    "        'type': 'user',\n",
    "        'role': 'writer',\n",
    "        'emailAddress': email\n",
    "    }\n",
    "    \n",
    "    request = drive_service.permissions().create(\n",
    "        fileId=file_id,\n",
    "        body=user_permission,\n",
    "        fields='id'\n",
    "    )\n",
    "    request.execute()\n",
    "\n",
    "def populate_google_doc(document_id, headers, general_data_row, related_queries_str):\n",
    "    requests = []\n",
    "    index = 1  # Initialize index for text\n",
    "\n",
    "    # Loop to add headers and their corresponding cell values\n",
    "    for header, value in zip(headers, general_data_row):\n",
    "        if header == \"Brief\":\n",
    "            continue  # Skip the 'Brief' header\n",
    "        \n",
    "        # Check if the value for each header is not empty\n",
    "        if value.strip():\n",
    "            requests.append({\n",
    "                'insertText': {\n",
    "                    'text': f\"{header}\\n\\n\",\n",
    "                    'location': {'index': index}\n",
    "                }\n",
    "            })\n",
    "            index += len(header) + 1  # Update index\n",
    "            \n",
    "            # Format Header as Title\n",
    "            requests.append({\n",
    "                'updateTextStyle': {\n",
    "                    'range': {\n",
    "                        'startIndex': index - len(header) - 1,\n",
    "                        'endIndex': index - 1,\n",
    "                    },\n",
    "                    'textStyle': {\n",
    "                        'bold': True,\n",
    "                        'fontSize': {\n",
    "                            'magnitude': 18,\n",
    "                            'unit': 'PT'\n",
    "                        }\n",
    "                    },\n",
    "                    'fields': 'bold,fontSize'\n",
    "                }\n",
    "            })\n",
    "\n",
    "            # Insert Cell Value (as normal paragraph)\n",
    "            requests.append({\n",
    "                'insertText': {\n",
    "                    'text': f\"{value}\\n\\n\",\n",
    "                    'location': {'index': index}\n",
    "                }\n",
    "            })\n",
    "            index += len(value) + 1  # Update index\n",
    "            \n",
    "            # Format Cell Value as Normal Paragraph\n",
    "            requests.append({\n",
    "                'updateTextStyle': {\n",
    "                    'range': {\n",
    "                        'startIndex': index - len(value) - 1,\n",
    "                        'endIndex': index - 1,\n",
    "                    },\n",
    "                    'textStyle': {\n",
    "                        'bold': False,\n",
    "                        'fontSize': {\n",
    "                            'magnitude': 12,\n",
    "                            'unit': 'PT'\n",
    "                        }\n",
    "                    },\n",
    "                    'fields': 'bold,fontSize'\n",
    "                }\n",
    "            })\n",
    "\n",
    "            # Add spacing after the paragraph using updateParagraphStyle\n",
    "            requests.append({\n",
    "                'updateParagraphStyle': {\n",
    "                    'range': {\n",
    "                        'startIndex': index - len(value) - 1,\n",
    "                        'endIndex': index\n",
    "                    },\n",
    "                    'paragraphStyle': {\n",
    "                        'spaceBelow': {\n",
    "                            'magnitude': 12,\n",
    "                            'unit': 'PT'\n",
    "                        }\n",
    "                    },\n",
    "                    'fields': 'spaceBelow'\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # Execute the batch update request\n",
    "    docs_service.documents().batchUpdate(documentId=document_id, body={'requests': requests}).execute()\n",
    "\n",
    "def main():\n",
    "    print(\"Starting main function\")\n",
    "\n",
    "    # Initialize the Chrome Service\n",
    "    service = Service(executable_path='/Users/juanpablocasadobissone/Downloads/chromedriver-mac-arm64 3/chromedriver')\n",
    "    chrome_options = Options()\n",
    "\n",
    "    keywords, content_types, rows = read_keywords_and_content_types(csv_file_path)\n",
    "\n",
    "\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    print(\"WebDriver initialized successfully.\")\n",
    "    search_url = f\"https://www.google.com/search\"\n",
    "    driver.get(search_url)\n",
    "    \n",
    "    doc_links = {}  # Dictionary to store document links for each keyword\n",
    "    time.sleep(25)\n",
    "    for keyword in keywords:\n",
    "        content_type = content_types.get(keyword, \"\")\n",
    "        if not content_type:\n",
    "            print(f\"Content type for keyword '{keyword}' not found. Skipping...\")\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "    \n",
    "        # Determine the corresponding sheet name\n",
    "        sheet_name = determine_sheet_name(content_type)\n",
    "    \n",
    "        # Read existing keywords from the sheet\n",
    "        existing_keywords = read_data_from_sheet(sheet_name)\n",
    "    \n",
    "        if keyword in existing_keywords:\n",
    "            print(f\"Keyword '{keyword}' already exists in sheet '{sheet_name}'. Skipping...\")\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        \n",
    "        # Call find_related_queries only for keywords that are not skipped\n",
    "        related_queries_for_keyword = find_related_queries(driver, keyword)\n",
    "        print(f\"Analyzing keyword: {keyword}\")\n",
    "\n",
    "        \n",
    "        content_type = content_types.get(keyword, \"\")\n",
    "        print(f\"Content type for keyword '{keyword}': {content_type}\")\n",
    "        \n",
    "        # Create and populate Google Doc\n",
    "        document_id = create_google_doc(f\"Content Brief for {keyword}\")\n",
    "        doc_link = f\"https://docs.google.com/document/d/{document_id}/edit\"\n",
    "        add_editor_to_doc(document_id, 'pablo@boyddigital.co.uk')\n",
    "        general_data_row = [keyword]  # Start the row with the keyword\n",
    "\n",
    "        # Update doc_links with the new Google Doc link\n",
    "        doc_links[keyword] = doc_link\n",
    "    \n",
    "        # Update the CSV file with the new link\n",
    "        update_csv_with_brief(csv_file_path, rows, doc_links)\n",
    "\n",
    "        if not content_type:\n",
    "            print(f\"Content type for keyword '{keyword}' not found. Skipping...\")\n",
    "            continue\n",
    "   \n",
    "        # Define sections for the blog\n",
    "        if \"blog\" in content_type.lower():\n",
    "            print(f\"Generating brief for keyword: {keyword}\")\n",
    "            # Define sections for the blog\n",
    "            sections = {\n",
    "                \"Title\": \"Provide a catchy title for the blog about '{keyword}'.\",\n",
    "                \"Meta Description\": \"Write a meta description for a blog post about '{keyword}'.\",\n",
    "                \"Introduction\": \"Create an engaging introduction for a blog about '{keyword}'.\",\n",
    "                \"Section 1\": \"Discuss the main aspects of '{keyword}'. Use bullet points.\",\n",
    "                \"Section 2\": \"Identify related topics to '{keyword}'.\",\n",
    "                \"Outro\": \"Write a compelling closing statement or call-to-action for the blog about '{keyword}'.\",\n",
    "            }\n",
    "            headers = [\"Keyword\", \"Title\", \"Meta Description\", \"Introduction\", \"Section 1\", \"Section 2\", \"Outro\", \"FAQ Questions\", \"FAQ Answers\", \"Brief\", \"Related Queries\"]        \n",
    "            sheet_name = \"Blog Sheet\"\n",
    "        \n",
    "            # Generating content for each blog section\n",
    "            for section_name, prompt_template in sections.items():\n",
    "                prompt = prompt_template.format(keyword=keyword)\n",
    "                content = generate_section(keyword, section_name, prompt)\n",
    "                general_data_row.append(content)  # Append each section content to general_data_row\n",
    "\n",
    "            # Generate FAQ questions\n",
    "            faq_question_prompt = \"Provide three frequently asked questions about {keyword}.\"\n",
    "            faq_questions = generate_faq_questions(keyword, faq_question_prompt.format(keyword=keyword))\n",
    "            \n",
    "            # Generate FAQ answers\n",
    "            faq_answer_prompt = \"Provide answers for the following frequently asked questions about {keyword}.\"\n",
    "            faq_answers = generate_faq_answers(keyword, faq_questions, faq_answer_prompt.format(keyword=keyword))\n",
    "            \n",
    "            # Append FAQ questions and answers to the data row as a single string for each\n",
    "            general_data_row.extend(['\\n'.join(faq_questions), '\\n'.join(faq_answers)])\n",
    "\n",
    "        elif \"branch\" in content_type.lower():\n",
    "            print(f\"Generating brief for keyword: {keyword}\")\n",
    "            sections = {\n",
    "                \"Title\": \"Generate a title for a branch page about '{keyword}'. Include branch location.\",\n",
    "                \"Meta Description\": \"Create a meta description for a branch page about '{keyword}' that includes key services and location.\",\n",
    "                \"Introduction\": \"Write a brief introduction about the branch for '{keyword}', highlighting its unique aspects.\",\n",
    "                \"Services/Products\": \"List the main services or products offered at the '{keyword}' branch.\",\n",
    "                \"Contact Information\": \"Suggest a format for presenting contact information for the '{keyword}' branch.\",\n",
    "                \"Local Insights\": \"Provide some local insights or interesting facts about the '{keyword}' branch's area.\",\n",
    "            }\n",
    "            headers = [\"Keyword\", \"Title\", \"Meta Description\", \"Introduction\", \"Services/Products\", \"Contact Information\", \"Local Insights\", \"Brief\", \"Related Queries\"]\n",
    "            sheet_name = \"Branch Sheet\"\n",
    "\n",
    "            for section_name, prompt_template in sections.items():\n",
    "                prompt = prompt_template.format(keyword=keyword)\n",
    "                content = generate_section(keyword, section_name, prompt)\n",
    "                general_data_row.append(content) \n",
    "\n",
    "        elif \"category\" in content_type.lower():\n",
    "            print(f\"Generating brief for keyword: {keyword}\")\n",
    "            sections = {\n",
    "                \"Title\": \"Generate a title for a category page focused on '{keyword}'.\",\n",
    "                \"Meta Description\": \"Create a meta description for a category page about '{keyword}', emphasizing the range of products or topics.\",\n",
    "                \"Introduction\": \"Write an engaging introduction for the category '{keyword}', giving an overview of what's included.\",\n",
    "                \"Key Features/Benefits\": \"List key features or benefits of the '{keyword}' category.\",\n",
    "                \"Items\": \"Suggest sub items relevant to the '{keyword}' category.\",\n",
    "            }\n",
    "            headers = [\"Keyword\", \"Title\", \"Meta Description\", \"Introduction\", \"Key Features/Benefits\", \"Sub items\", \"Brief\", \"Related Queries\"]\n",
    "            sheet_name = \"Category Sheet\"\n",
    "\n",
    "            for section_name, prompt_template in sections.items():\n",
    "                prompt = prompt_template.format(keyword=keyword)\n",
    "                content = generate_section(keyword, section_name, prompt)\n",
    "                general_data_row.append(content) \n",
    "\n",
    "        elif \"landing\" in content_type.lower():\n",
    "            print(f\"Generating brief for keyword: {keyword}\")\n",
    "            sections = {\n",
    "                \"Title\": \"Create a compelling title for a landing page about '{keyword}'.\",\n",
    "                \"Meta Description\": \"Write a meta description for the landing page focused on '{keyword}', highlighting the main offer or call-to-action.\",\n",
    "                \"Hero Section Idea\": \"Suggest an idea for the hero section content for '{keyword}', including a value proposition.\",\n",
    "                \"Key Benefits\": \"List the key benefits or features to highlight on the landing page for '{keyword}'.\",\n",
    "                \"CTA Suggestions\": \"Provide suggestions for effective call-to-action phrases for '{keyword}'.\",\n",
    "                \"Testimonial/Review Highlights\": \"Outline how to incorporate testimonials or reviews for '{keyword}'.\",\n",
    "            }\n",
    "            headers = [\"Keyword\", \"Title\", \"Meta Description\", \"Hero Section Idea\", \"Key Benefits\", \"CTA Suggestions\", \"Testimonial/Review Highlights\", \"Brief\", \"Related Queries\"]\n",
    "            sheet_name = \"Landing Page Sheet\"\n",
    "\n",
    "            for section_name, prompt_template in sections.items():\n",
    "                prompt = prompt_template.format(keyword=keyword)\n",
    "                content = generate_section(keyword, section_name, prompt)\n",
    "                general_data_row.append(content) \n",
    "\n",
    "        elif \"pr article\" in content_type.lower():\n",
    "            print(f\"Generating brief for keyword: {keyword}\")\n",
    "            sections = {\n",
    "                \"Title\": \"Generate a title for the PR article about '{keyword}'.\",\n",
    "                \"Meta Description\": \"Create a meta description for the PR article about '{keyword}', highlighting its key points.\",\n",
    "                \"Introduction\": \"Write an engaging introduction for the PR article about '{keyword}', providing context.\",\n",
    "                \"Key Points\": \"List the key points to cover in the PR article about '{keyword}'.\",\n",
    "                \"CTA\": \"Suggest a call-to-action for the PR article, encouraging readers to take a specific action.\",\n",
    "            }\n",
    "            headers = [\"Keyword\", \"Title\", \"Meta Description\", \"Introduction\", \"Key Points\", \"CTA\", \"Brief\", \"Related Queries\"]\n",
    "            sheet_name = \"PR Article Sheet\"\n",
    "\n",
    "            for section_name, prompt_template in sections.items():\n",
    "                prompt = prompt_template.format(keyword=keyword)\n",
    "                content = generate_section(keyword, section_name, prompt)\n",
    "                general_data_row.append(content) \n",
    "\n",
    "        elif \"intent page\" in content_type.lower():\n",
    "            print(f\"Generating brief for keyword: {keyword}\")\n",
    "            sections = {\n",
    "                \"Title\": \"Generate a title for the intent page about '{keyword}'.\",\n",
    "                \"Meta Description\": \"Create a meta description for the intent page about '{keyword}', emphasizing the user's intent.\",\n",
    "                \"Introduction\": \"Write an introduction for the intent page about '{keyword}', addressing the user's needs.\",\n",
    "                \"User Intent\": \"Define the user's intent and goals when visiting the intent page for '{keyword}'.\",\n",
    "                \"Key Features\": \"List the key features or information that the intent page should provide to fulfill the user's intent.\",\n",
    "                \"CTA\": \"Suggest a call-to-action for the intent page, guiding users toward their desired action.\",\n",
    "            }\n",
    "            headers = [\"Keyword\", \"Title\", \"Meta Description\", \"Introduction\", \"User Intent\", \"Key Features\", \"CTA\", \"Brief\", \"Related Queries\"]\n",
    "            sheet_name = \"Intent Page Sheet\"\n",
    "\n",
    "            for section_name, prompt_template in sections.items():\n",
    "                prompt = prompt_template.format(keyword=keyword)\n",
    "                content = generate_section(keyword, section_name, prompt)\n",
    "                general_data_row.append(content) \n",
    "    \n",
    "        elif \"product\" in content_type.lower():\n",
    "            print(f\"Generating brief for keyword: {keyword}\")\n",
    "            sections = {\n",
    "                \"Title\": \"Generate a product title for '{keyword}', focusing on its main feature.\",\n",
    "                \"Meta Description\": \"Create a concise meta description for the product '{keyword}', emphasizing its unique selling points.\",\n",
    "                \"Feature Highlights\": \"List the top features of the product '{keyword}'.\",\n",
    "                \"Benefit Angles\": \"Provide angles on how to present the benefits of '{keyword}' to the customer.\",\n",
    "                \"CTA Approach\": \"Suggest an effective call-to-action for encouraging purchases of '{keyword}'.\",\n",
    "            }\n",
    "            headers = [\"Keyword\", \"Title\", \"Meta Description\", \"Feature Highlights\", \"Benefit Angles\", \"CTA Approach\", \"Brief\", \"Related Queries\"]\n",
    "            sheet_name = \"Product Sheet\"\n",
    "\n",
    "            for section_name, prompt_template in sections.items():\n",
    "                prompt = prompt_template.format(keyword=keyword)\n",
    "                content = generate_section(keyword, section_name, prompt)\n",
    "                general_data_row.append(content) \n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid content type\")\n",
    "        \n",
    "        print(f\"Analyzing keyword: {keyword}, Content Type: {content_type}, Sheet Name: {sheet_name}\")\n",
    "\n",
    "        # Add the document link and related queries to the data row\n",
    "        general_data_row.extend([doc_link, related_queries_for_keyword])\n",
    "\n",
    "        # Write all the data to the Google Sheet and update CSV\n",
    "        write_to_sheet(sheet_name, headers, [general_data_row])\n",
    "        populate_google_doc(document_id, headers, general_data_row, related_queries_for_keyword)\n",
    "        update_csv_with_brief(csv_file_path, rows, doc_links)\n",
    "\n",
    "        print(f\"All tasks completed for keyword: {keyword}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f0aea7c-8077-49f7-b33b-03a729776ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(refined_content)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 71\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m---> 71\u001b[0m     keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter the keyword to search for in news articles: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     matching_articles \u001b[38;5;241m=\u001b[39m fetch_news_with_beautifulsoup(keyword)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matching_articles:\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "#Newsjack script\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from goose3 import Goose\n",
    "\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Define the URL of the news website\n",
    "URL = \"https://www.bbc.com/news\"\n",
    "\n",
    "def fetch_news_with_beautifulsoup(keyword):\n",
    "    \"\"\"Fetch news headlines and links from BBC.\"\"\"\n",
    "    response = requests.get(URL)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve webpage.\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    headlines = soup.find_all('h3', class_='gs-c-promo-heading__title')\n",
    "\n",
    "    matching_articles = []\n",
    "    for headline in headlines:\n",
    "        if keyword.lower() in headline.text.lower():\n",
    "            # Find the closest anchor tag within the headline's parents/siblings\n",
    "            anchor = headline.find_previous(\"a\", href=True)\n",
    "            if anchor:\n",
    "                matching_articles.append((headline.text, anchor['href']))\n",
    "            else:\n",
    "                print(f\"Anchor not found for headline: {headline.text}\")\n",
    "\n",
    "    return matching_articles\n",
    "\n",
    "\n",
    "\n",
    "def extract_article_content_with_goose(article_link):\n",
    "    \"\"\"Extract content of the specified news article using Goose.\"\"\"\n",
    "    full_url = \"https://www.bbc.com\" + article_link\n",
    "    g = Goose()\n",
    "    article = g.extract(url=full_url)\n",
    "    return article.cleaned_text\n",
    "\n",
    "def create_draft(article_content):\n",
    "    \"\"\"Generate a rough draft rewriting of the provided content.\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Provide a rough draft rewriting of the following content: \\\"{article_content}\\\"\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "\n",
    "def refine_draft(draft_content):\n",
    "    \"\"\"Refine the draft to make it more polished and engaging.\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Refine the following draft to make it more polished and engaging: \\\"{draft_content}\\\"\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    keyword = input(\"Enter the keyword to search for in news articles: \")\n",
    "    matching_articles = fetch_news_with_beautifulsoup(keyword)\n",
    "\n",
    "    if not matching_articles:\n",
    "        print(f\"No news items found with keyword '{keyword}'.\")\n",
    "        return\n",
    "\n",
    "    # Taking the first matched article for simplicity.\n",
    "    title, link = matching_articles[0]\n",
    "    print(f\"Found an article with title: {title}\")\n",
    "    print(f\"Extracting content from: {link}\")\n",
    "\n",
    "    article_content = extract_article_content(link)\n",
    "    if not article_content:\n",
    "        print(\"Failed to extract article content.\")\n",
    "        return\n",
    "\n",
    "    # Step 1: Create Draft\n",
    "    draft = create_draft(article_content)\n",
    "    print(\"\\nGenerated Draft:\")\n",
    "    print(draft)\n",
    "\n",
    "    # Step 2: Refine Draft\n",
    "    refined_content = refine_draft(draft)\n",
    "    print(\"\\nRefined Content:\")\n",
    "    print(refined_content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9c46e43a-65c8-4f17-b85c-3f8fe567f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing keyword: Keyword\n",
      "No news items found with keyword 'Keyword'.\n",
      "Processing keyword: Israel\n",
      "Found an article with title: Gaza's only power plant runs out of fuel during Israeli siege\n",
      "Extracting content from: /news/live/world-middle-east-67073970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Publish date Video 4 minutes 38 seconds4:38 could not be resolved to UTC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Draft:\n",
      "I am currently situated on the border between Lebanon and Israel, specifically in northern Israel. Earlier, there were reports circulating about infiltrations coming from Lebanon.\n",
      "\n",
      "I happened to be present in one of the communities along the border during this time. The local residents have taken precautions and armed themselves in anticipation of any potential infiltrations by Hezbollah.\n",
      "\n",
      "Speculation arose suggesting that there may have been an aerial infiltration, particularly involving paragliders. However, the Israeli military thoroughly investigated the situation and concluded that it was a false alarm. Nevertheless, this incident highlights the prevailing sense of unease in the area.\n",
      "\n",
      "The northern border holds significant importance, as nobody desires for the conflict to escalate and spread throughout the region. This fear is not limited to the local population but is also shared internationally.\n",
      "\n",
      "Refined Content:\n",
      "I am currently in the northern region of Israel, which is located on the border with Lebanon. Recently, there were reports circulating about potential infiltrations from Lebanon.\n",
      "\n",
      "During this time, I happened to be in a community along the border. The local residents have taken precautions and armed themselves in anticipation of any potential infiltrations by a group.\n",
      "\n",
      "There were speculations suggesting that there might have been an aerial infiltration, specifically involving paragliders. However, the Israeli military thoroughly investigated the situation and determined that it was a false alarm. Nonetheless, this incident highlights the prevailing sense of unease in the area.\n",
      "\n",
      "The northern border holds significant importance, as everyone is concerned about avoiding any escalation of the conflict and its potential spread throughout the region. This fear is not only felt by the local population but is also shared internationally.\n",
      "Document saved at: https://docs.google.com/document/d/1RXHQ58W4YhIxEnayHf2Mzlypv99QHMcjtT1QkRjHgQ8/edit\n",
      "Updated spreadsheet row 2 with source URL and document link.\n",
      "Processing keyword: Messi\n",
      "Found an article with title: Messi will not go on loan after MLS season - Balague\n",
      "Extracting content from: /sport/football/67076480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Publish date 4 hours ago4 hours ago could not be resolved to UTC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Draft:\n",
      "The latest update on Lionel Messi's future reveals that he will not be joining another team on loan once the Major League Soccer season concludes. According to Spanish football expert Guillem Balague, Messi's current team, Inter Miami, is no longer in contention for the playoffs after their recent loss to FC Cincinnati. Their season will officially end on 21 October.\n",
      "\n",
      "Due to an injury, the Argentine World Cup winner has only managed to play 72 minutes in the MLS since early September. Following the conclusion of the regular MLS season, Messi is slated to participate in World Cup qualifiers for Argentina against Uruguay and Brazil on 17 and 22 November. Afterward, he is expected to take a one-month holiday, like many other footballers.\n",
      "\n",
      "Balague dispelled any notions of Messi considering a move to Saudi Arabia or any similar destination during this break. Instead, Messi plans to return to Miami for pre-season training, as the new MLS season is set to commence in February.\n",
      "\n",
      "Since his arrival at Inter Miami from Paris St-Germain in July, Messi has led the team to their first Leagues Cup victory in August, although they fell short in the US Open Cup. Despite scoring 11 goals in 13 games for the club, only one of those goals was scored in his five MLS appearances.\n",
      "\n",
      "Speculation about a potential return to Barcelona, his former club of 21 years before joining PSG, has persisted since his departure in 2021. However, Inter Miami manager Tata Martino expressed surprise when questioned about these rumors after the loss to FC Cincinnati, claiming to have no information regarding the matter.\n",
      "\n",
      "Please note that we have enhanced our coverage of your favorite Premier League club on the BBC Sport app, ensuring that you receive notifications and stay up to date with all the latest news and moments.\n",
      "\n",
      "Refined Content:\n",
      "The latest update on Lionel Messi's future reveals that he will not be joining another team on loan once the Major League Soccer season concludes. According to a well-known football expert, Messi's current team, Inter Miami, is no longer in contention for the playoffs after their recent loss. Their season will officially end on 21 October.\n",
      "\n",
      "Due to an injury, the Argentine World Cup winner has only managed to play 72 minutes in the MLS since early September. Following the conclusion of the regular MLS season, Messi is slated to participate in World Cup qualifiers for Argentina against Uruguay and Brazil on 17 and 22 November. Afterward, he is expected to take a one-month holiday, like many other footballers.\n",
      "\n",
      "The expert dispelled any notions of Messi considering a move to a specific destination during this break. Instead, Messi plans to return to Miami for pre-season training, as the new MLS season is set to commence in February.\n",
      "\n",
      "Since his arrival at Inter Miami from a previous club in July, Messi has led the team to their first victory in August, although they fell short in another cup competition. Despite his impressive goal tally, scoring 11 goals in 13 games for the club, only one of those goals was scored in his five MLS appearances.\n",
      "\n",
      "Speculation about a potential return to his former club of 21 years has persisted since his departure. However, Inter Miami's manager expressed surprise when questioned about these rumors after the recent loss, claiming to have no information regarding the matter.\n",
      "\n",
      "Please note that there have been enhancements to the coverage of your favorite Premier League club on a sports app, ensuring that you receive notifications and stay up to date with all the latest news and moments.\n",
      "Document saved at: https://docs.google.com/document/d/1ZMRKj7qqJ9j2JZcuoJGHwAbqkkJhFA7Eo854YFfrfSo/edit\n",
      "Updated spreadsheet row 3 with source URL and document link.\n",
      "Processing keyword: Dupont\n",
      "Found an article with title: Dupont 'at 100%' as he targets France return\n",
      "Extracting content from: /sport/rugby-union/67083117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Publish date 1 hour ago1 hour ago could not be resolved to UTC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Draft:\n",
      "Last updated on . From the section Rugby Union\n",
      "\n",
      "The French rugby player Antoine Dupont has been actively participating in training sessions and is fully fit ahead of a potential return to the Rugby World Cup, according to France's forwards coach, William Servat.\n",
      "\n",
      "Dupont received clearance to resume full training on Monday and could possibly be included in the team's quarter-final match against South Africa in Paris on Sunday.\n",
      "\n",
      "The team's head coach, Fabien Galthie, is set to announce the lineup for the game on Friday.\n",
      "\n",
      "According to Servat, \"Antoine is progressing excellently, he needed some rest. He has been actively contributing to the team's strategic decisions and the adjustments we are making to our game plan.\n",
      "\n",
      "\"Today, he was even more involved. He is back with us and fully engaged in training.\"\n",
      "\n",
      "The Springboks, who won the World Cup in 2019, are anticipated to offer a tough physical challenge for the hosts in the quarter-final clash.\n",
      "\n",
      "When asked whether France would need to revise their defensive strategies if Dupont is selected and concerned about his previous injury, Servat responded, \"Antoine is one of our best defenders, and he is 100% fit. There is no reason to make any changes.\"\n",
      "\n",
      "France won all their group stage matches to top Pool A, while South Africa finished second in Pool B behind Ireland to secure a spot in the quarter-finals.\n",
      "\n",
      "Refined Content:\n",
      "Updated version:\n",
      "\n",
      "As of the latest update, the French rugby player Antoine Dupont is fully fit and actively participating in training sessions. According to France's forwards coach, William Servat, Dupont has received clearance to return to full training and might be included in the team's upcoming quarter-final match against South Africa in Paris on Sunday.\n",
      "\n",
      "The team's head coach, Fabien Galthie, is expected to announce the lineup for the game on Friday.\n",
      "\n",
      "Servat noted that Dupont's progress has been excellent and he has actively contributed to the team's strategic decisions and game plan adjustments. He further emphasized Dupont's increased involvement and full engagement in training.\n",
      "\n",
      "South Africa, the reigning World Cup champions, are anticipated to provide a formidable physical challenge for the host team in the quarter-final clash.\n",
      "\n",
      "When asked about the possibility of revising defensive strategies if Dupont is selected due to concerns about his previous injury, Servat responded confidently stating that Dupont is one of their best defenders and is fully fit, hence no changes are necessary.\n",
      "\n",
      "France emerged victorious in all their group stage matches to top Pool A, while South Africa secured a quarter-final spot by finishing second in Pool B behind Ireland.\n",
      "Document saved at: https://docs.google.com/document/d/1ixn5M2LoeLDhjbjD0LeQa_f45z2kQA0CoF8lw6u5rnA/edit\n",
      "Updated spreadsheet row 4 with source URL and document link.\n",
      "Processing keyword: Egypt\n",
      "Found an article with title: Egypt warned Israel days before Hamas struck - US\n",
      "Extracting content from: /news/world-middle-east-67082047\n",
      "\n",
      "Generated Draft:\n",
      "During a closed-door intelligence briefing, Mr. McCaul stated that Egypt had warned the Israelis about a potential event similar to the current crisis in the Middle East. He mentioned that a warning was given, but did not provide specific details regarding the level of the warning. An Egyptian intelligence official revealed that they had repeatedly warned Israel about an imminent and significant event planned from Gaza. However, the official stated that Israeli officials seemed to downplay the threat from Gaza, focusing instead on the West Bank. According to unnamed officials, there was no concrete intelligence regarding a specific attack. Mr. Netanyahu denied receiving any specific warning prior to the deadly incursion, labeling such claims as \"totally fake news.\" The death toll in Israel has reached 1,200 from Hamas attacks, whereas over 1,000 people have been killed by Israeli air strikes in Gaza. Israel has responded to the Hamas attacks by targeting their positions in Gaza, causing residents to face power outages due to the depletion of fuel in their only power station.\n",
      "\n",
      "Refined Content:\n",
      "During a confidential intelligence briefing, Mr. McCaul conveyed that Egypt had issued a caution to the Israelis about a potential event comparable to the ongoing crisis in the Middle East. However, he did not disclose the specifics of the warning. An Egyptian intelligence official disclosed that they had recurrently alerted Israel about an imminent and significant event planned from Gaza. Nevertheless, the official noted that Israeli officials seemed to underestimate the threat from Gaza, focusing their attention on the West Bank instead. Unnamed officials reported that there was no concrete intelligence pertaining to a specific attack. In response to claims of receiving prior warnings before the deadly incursion, Mr. Netanyahu dismissed such allegations as unfounded. The loss of life in Israel due to Hamas attacks has risen to 1,200, while Israeli air strikes in Gaza have resulted in the deaths of over 1,000 people. Israel has retaliated against Hamas attacks by targeting their positions in Gaza, leading to power outages for residents caused by the depletion of fuel at their sole power station.\n",
      "Document saved at: https://docs.google.com/document/d/1_XC8pRQ5X1I5MNVCCuM0HBddE5R4OR5F884u4RQ4WBY/edit\n",
      "Updated spreadsheet row 5 with source URL and document link.\n",
      "Processing keyword: India\n",
      "Found an article with title: Rohit century races India to win over Afghanistan\n",
      "Extracting content from: /sport/cricket/67078199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Publish date 2 hours ago2 hours ago could not be resolved to UTC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Draft:\n",
      "Last updated on . From the section Cricket\n",
      "\n",
      "Captain [Player's Name]'s impressive century led India to a resounding eight-wicket victory over Afghanistan in the World Cup match held in Delhi.\n",
      "\n",
      "Hashmatullah Shahidi scored 80 runs to set a challenging target for Afghanistan, but Jasprit Bumrah's excellent bowling figures of 4-39 restricted their final score to 272-8.\n",
      "\n",
      "This loss marks Afghanistan's second defeat in two games, while India has secured two wins.\n",
      "\n",
      "After two matches, the hosts, along with New Zealand and Pakistan, maintain a perfect record. They will now face their fierce rivals, Pakistan, in the highly anticipated match of the tournament in Ahmedabad on Saturday.\n",
      "\n",
      "Afghanistan will stay in Delhi to play against defending champions England on Sunday.\n",
      "\n",
      "Rohit Sharma, who was the leading run-scorer in the previous World Cup in 2019, missed out on scoring in India's opening win over Australia. However, his remarkable performance against Afghanistan was outstanding and record-breaking.\n",
      "\n",
      "Rohit scored his seventh World Cup century, surpassing India legend Sachin Tendulkar's six centuries. He achieved this milestone by reaching three figures from just 63 balls, making it the fastest hundred by an Indian in a World Cup match, surpassing Kapil Dev's famous 72-ball effort in 1983. Rohit also hit his 554th international cricket six, overtaking West Indies' Chris Gayle's previous record.\n",
      "\n",
      "His innings was a perfect blend of elegance and powerful hitting. Rohit took advantage of Afghanistan's short deliveries with pull shots and drove through or over the off side when they overpitched the ball. His opening partner, Ishan Kishan, contributed 14 runs to their century opening stand.\n",
      "\n",
      "After Kishan was dismissed for 47, Rohit continued to build partnerships, adding another 49 runs with Virat Kohli. Eventually, Rohit fell to a sweep shot against Rashid Khan, but by then, India needed only 68 runs for victory.\n",
      "\n",
      "Kohli remained unbeaten on 55, following his impressive knock of 85 against Australia, making India a formidable team in the tournament.\n",
      "\n",
      "Although Afghanistan suffered a one-sided defeat to Bangladesh, they showed signs of competitiveness in this match. Skipper Hashmatullah played a crucial role and formed a 121-run partnership for the fourth wicket with Azmatullah Omarzai. Hashmatullah elegantly guided the ball towards the third man, while Azmatullah displayed powerful shots down the ground.\n",
      "\n",
      "With Afghanistan at 184-3 in the 35th over, India started to show signs of desperation, but Azmatullah's dismissal for 62 off a slower ball from Hardik Pandya halted their progress. Subsequently, the Afghan innings faltered, and wickets fell regularly, including Hashmatullah's dismissal when he was leg before wicket while attempting a sweep against Kuldeep Yadav. With Bumrah in top form, Afghanistan could only add 83 runs in the final 15 overs.\n",
      "\n",
      "Considering the ease of batting due to the dew under the floodlights, the total was never going to be a challenge for India. Even though Afghanistan strangely delayed their star leg-spinner Rashid's entry into the attack until the 15th over, by that time, Rohit had already inflicted significant damage.\n",
      "\n",
      "In truth, nothing could stop the remarkable Rohit's brilliance.\n",
      "\n",
      "Reacting to his performance, India captain Rohit Sharma stated, \"It is always special to score a hundred in a World Cup. I don't want to think about records too much. There is a long way to go, and I don't want to lose my focus. It is very important to get momentum at the start and have the winning streak continue.\"\n",
      "\n",
      "Afghanistan captain Hashmatullah Shahidi said, \"We had a target of 300+ in these conditions, especially as India's batting lineup is strong. But back-to-back wickets cost us. We will try and learn from our mistakes and come back with a positive attitude.\"\n",
      "\n",
      "Refined Content:\n",
      "On __ (date), India secured a resounding eight-wicket victory over Afghanistan in a World Cup match held in Delhi. Captain [Player's Name] played a remarkable innings, scoring a century that led India to the win.\n",
      "\n",
      "Afghanistan set a challenging target with Hashmatullah Shahidi scoring 80 runs. However, Jasprit Bumrah's excellent bowling figures of 4-39 restricted their final score to 272-8.\n",
      "\n",
      "This loss marks Afghanistan's second defeat in two games, while India has secured two wins. Along with New Zealand and Pakistan, the hosts maintain a perfect record after two matches. The highly anticipated match against Pakistan will take place in Ahmedabad on Saturday.\n",
      "\n",
      "Afghanistan will stay in Delhi to play against defending champions England on Sunday.\n",
      "\n",
      "Rohit Sharma, who was the leading run-scorer in the previous World Cup in 2019, had missed out on scoring in India's opening win over Australia. However, his performance against Afghanistan was outstanding and record-breaking.\n",
      "\n",
      "Rohit scored his seventh World Cup century, surpassing India legend Sachin Tendulkar's six centuries. He achieved this milestone by reaching three figures from just 63 balls, making it the fastest hundred by an Indian in a World Cup match, surpassing Kapil Dev's famous 72-ball effort in 1983. Rohit also broke the record for the most international cricket sixes by an Indian, hitting his 554th, surpassing the previous record set by West Indies' Chris Gayle.\n",
      "\n",
      "His innings demonstrated a perfect mix of elegance and power. Rohit capitalized on Afghanistan's short deliveries with pull shots and drove through or over the off side when they overpitched the ball. Opening partner Ishan Kishan also contributed 14 runs to their century opening stand.\n",
      "\n",
      "After Kishan was dismissed for 47, Rohit continued to build partnerships, adding another 49 runs with Virat Kohli. Eventually, Rohit fell to a sweep shot against Rashid Khan, but by then, India needed only 68 runs for victory.\n",
      "\n",
      "Kohli remained unbeaten on 55, following his impressive knock of 85 against Australia.\n",
      "\n",
      "Although Afghanistan suffered a one-sided defeat to Bangladesh, they showed signs of competitiveness in this match. Skipper Hashmatullah played a crucial role and formed a 121-run partnership for the fourth wicket with Azmatullah Omarzai. Hashmatullah elegantly guided the ball towards the third man, while Azmatullah displayed powerful shots down the ground.\n",
      "\n",
      "With Afghanistan at 184-3 in the 35th over, India started to show signs of desperation, but Azmatullah's dismissal for 62 off a slower ball from Hardik Pandya halted their progress. Subsequently, the Afghan innings faltered, and wickets fell regularly, including Hashmatullah's dismissal when he was leg before wicket while attempting a sweep against Kuldeep Yadav. With Bumrah in top form, Afghanistan could only add 83 runs in the final 15 overs.\n",
      "\n",
      "Considering the ease of batting due to the dew under the floodlights, the total was never going to be a challenge for India. Even though Afghanistan strangely delayed their star leg-spinner Rashid's entry into the attack until the 15th over, by that time, Rohit had already inflicted significant damage.\n",
      "\n",
      "In truth, nothing could stop Rohit's brilliance.\n",
      "\n",
      "Reacting to his performance, India captain Rohit Sharma stated, \"It is always special to score a hundred in a World Cup. I don't want to think about records too much. There is a long way to go, and I don't want to lose my focus. It is very important to get momentum at the start and have the winning streak continue.\"\n",
      "\n",
      "Afghanistan captain Hashmatullah Shahidi said, \"We had a target of 300+ in these conditions, especially as India's batting lineup is strong. But back-to-back wickets cost us. We will try and learn from our mistakes and come back with a positive attitude.\"\n",
      "Document saved at: https://docs.google.com/document/d/14EeyB3Y0nwJy0UWAI9vm4QBaAjlBn5dM5RP31ngQkcs/edit\n",
      "Updated spreadsheet row 6 with source URL and document link.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from goose3 import Goose\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "scopes = [\n",
    "    'https://www.googleapis.com/auth/spreadsheets',\n",
    "    'https://www.googleapis.com/auth/documents',\n",
    "    'https://www.googleapis.com/auth/drive'\n",
    "]\n",
    "\n",
    "# Initialize the credentials with the defined scopes\n",
    "credentials = Credentials.from_service_account_file('/Users/juanpablocasadobissone/Downloads/boyd-digital-scripts-50288f948562.json', scopes=scopes)\n",
    "\n",
    "# Initialize the Drive API client\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# Initialize the Sheets API client\n",
    "sheets_service = build('sheets', 'v4', credentials=credentials)\n",
    "\n",
    "# Initialize the Docs API client\n",
    "docs_service = build('docs', 'v1', credentials=credentials)\n",
    "\n",
    "# Define the URL of the news website\n",
    "URL = \"https://www.bbc.com/news\"\n",
    "\n",
    "def fetch_news_with_beautifulsoup(keyword):\n",
    "    \"\"\"Fetch news headlines and links from BBC.\"\"\"\n",
    "    response = requests.get(URL)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve webpage.\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    headlines = soup.find_all('h3', class_='gs-c-promo-heading__title')\n",
    "\n",
    "    matching_articles = []\n",
    "    for headline in headlines:\n",
    "        if keyword.lower() in headline.text.lower():\n",
    "            # Find the closest anchor tag within the headline's parents/siblings\n",
    "            anchor = headline.find_previous(\"a\", href=True)\n",
    "            if anchor:\n",
    "                matching_articles.append((headline.text, anchor['href']))\n",
    "            else:\n",
    "                print(f\"Anchor not found for headline: {headline.text}\")\n",
    "\n",
    "    return matching_articles\n",
    "\n",
    "\n",
    "\n",
    "def extract_article_content_with_goose(article_link):\n",
    "    \"\"\"Extract content of the specified news article using Goose.\"\"\"\n",
    "    full_url = \"https://www.bbc.com\" + article_link\n",
    "    g = Goose()\n",
    "    article = g.extract(url=full_url)\n",
    "    return article.cleaned_text\n",
    "\n",
    "def create_draft(article_content):\n",
    "    \"\"\"Generate a rough draft rewriting of the provided content.\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Provide a rough draft rewriting of the following content without mentioning any specific publisher brands: \\\"{article_content}\\\"\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "def refine_draft(draft_content):\n",
    "    \"\"\"Refine the draft to make it more polished and engaging.\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Refine the following draft without referencing any specific publisher brands: \\\"{draft_content}\\\"\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "\n",
    "def save_to_google_docs(original_content, refined_content):\n",
    "    # Create a new Google Doc\n",
    "    document = docs_service.documents().create().execute()\n",
    "\n",
    "    # Set permission for the Doc\n",
    "    set_file_permission(document[\"documentId\"])\n",
    "\n",
    "    # Define content structure for Google Doc\n",
    "    requests = [\n",
    "        {\n",
    "            'insertText': {\n",
    "                'location': {\n",
    "                    'index': 1,\n",
    "                },\n",
    "                'text': f\"Original Content:\\n{original_content}\\n\\nRefined Content:\\n{refined_content}\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    docs_service.documents().batchUpdate(documentId=document[\"documentId\"], body={\"requests\": requests}).execute()\n",
    "    \n",
    "    return f\"https://docs.google.com/document/d/{document['documentId']}/edit\"\n",
    "\n",
    "def set_file_permission(file_id):\n",
    "    # Define the permissions to set the file as \"Anyone with the link can edit\"\n",
    "    permissions = {\n",
    "        'role': 'writer',\n",
    "        'type': 'anyone'\n",
    "    }\n",
    "    # Apply the permissions to the file\n",
    "    drive_service.permissions().create(fileId=file_id, body=permissions).execute()\n",
    "\n",
    "\n",
    "# Google Sheet ID and range (assuming keywords are in column A)\n",
    "SPREADSHEET_ID = '1sOSi15HJeFUY50s4DPKK6knv9HBEB7C5NhnUF5eY_YA'\n",
    "RANGE_NAME = 'Newsjacks!A:A'  # Adjust the range if necessary\n",
    "\n",
    "def get_keywords_from_sheet():\n",
    "    \"\"\"Fetch the list of keywords from the Google Sheet.\"\"\"\n",
    "    result = sheets_service.spreadsheets().values().get(spreadsheetId=SPREADSHEET_ID, range=RANGE_NAME).execute()\n",
    "    values = result.get('values', [])\n",
    "    keywords = [row[0] for row in values]\n",
    "    return keywords\n",
    "\n",
    "def update_sheet_with_data(row_number, keyword, source_url, doc_link):\n",
    "    \"\"\"Update the Google Sheet with keyword, source URL, and Google Doc link.\"\"\"\n",
    "    range_name = f\"Newsjacks!A{row_number}:C{row_number}\"\n",
    "    values = [[keyword, source_url, doc_link]]\n",
    "    body = {\n",
    "        \"values\": values\n",
    "    }\n",
    "    sheets_service.spreadsheets().values().update(spreadsheetId=SPREADSHEET_ID, range=range_name, valueInputOption=\"RAW\", body=body).execute()\n",
    "\n",
    "\n",
    "def main():\n",
    "    keywords = get_keywords_from_sheet()\n",
    "\n",
    "    for idx, keyword in enumerate(keywords, 1):  # start index at 1 to match row numbers\n",
    "        print(f\"Processing keyword: {keyword}\")\n",
    "        matching_articles = fetch_news_with_beautifulsoup(keyword)\n",
    "\n",
    "        if not matching_articles:\n",
    "            print(f\"No news items found with keyword '{keyword}'.\")\n",
    "            continue\n",
    "\n",
    "        # Taking the first matched article for simplicity.\n",
    "        title, link = matching_articles[0]\n",
    "        print(f\"Found an article with title: {title}\")\n",
    "        print(f\"Extracting content from: {link}\")\n",
    "\n",
    "        article_content = extract_article_content(link)\n",
    "        if not article_content:\n",
    "            print(\"Failed to extract article content.\")\n",
    "            continue\n",
    "\n",
    "        # Step 1: Create Draft\n",
    "        draft = create_draft(article_content)\n",
    "        print(\"\\nGenerated Draft:\")\n",
    "        print(draft)\n",
    "\n",
    "        # Step 2: Refine Draft\n",
    "        refined_content = refine_draft(draft)\n",
    "        print(\"\\nRefined Content:\")\n",
    "        print(refined_content)\n",
    "\n",
    "        # Save to Google Docs\n",
    "        doc_link = save_to_google_docs(article_content, refined_content)\n",
    "        \n",
    "        # Update the Google Sheet with source URL and Google Doc link\n",
    "        update_sheet_with_data(idx, keyword, URL + link, doc_link)\n",
    "\n",
    "        print(f\"Document saved at: {doc_link}\")\n",
    "        print(f\"Updated spreadsheet row {idx} with source URL and document link.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5146d-5e60-48f0-a3b6-cabdfa1fad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraper\n",
    "#url scraper\n",
    "\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "\n",
    "# User Agent string to be used for making requests\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.3\"\n",
    "\n",
    "\n",
    "# Function to append URLs to a CSV file\n",
    "def append_to_csv(data, filename):\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in data:\n",
    "            writer.writerow([row])\n",
    "\n",
    "# Function to load the last URL from the CSV file\n",
    "def load_last_url(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            last_row = None\n",
    "            for last_row in reader:\n",
    "                pass\n",
    "            if last_row:\n",
    "                return last_row[0]\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_visited_urls(filename):\n",
    "    visited = set()\n",
    "    try:\n",
    "        with open(filename, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for row in reader:\n",
    "                if row:  # check if row is not empty\n",
    "                    visited.add(row[0])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    return visited\n",
    "\n",
    "\n",
    "\n",
    "# Function to fetch and parse HTML content\n",
    "def fetch_html(url):\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to find and filter URLs\n",
    "def extract_and_filter_urls(html, base_url, visited, flexible_target_domain):\n",
    "    base_domain = urlparse(base_url).netloc\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    urls = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    filtered_urls = []\n",
    "    \n",
    "    for url in urls:\n",
    "        full_url = urljoin(base_url, url)\n",
    "        url_domain = urlparse(full_url).netloc\n",
    "        \n",
    "        if url_domain not in flexible_target_domain:\n",
    "            continue\n",
    "            \n",
    "        if \"cdn-cgi\" in url or \"#\" in url or \"page\" in url or \"?s=\" in url or \"?\" in url:\n",
    "            continue\n",
    "            \n",
    "        if full_url not in visited:\n",
    "            filtered_urls.append(full_url)\n",
    "    \n",
    "    return filtered_urls\n",
    "\n",
    "\n",
    "# Function to save URLs to a CSV file\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['URL'])\n",
    "        for row in data:\n",
    "            writer.writerow([row])\n",
    "\n",
    "# Modify this part of your code\n",
    "def crawl(base_url, csv_filename, flexible_target_domain):\n",
    "    visited = load_visited_urls(csv_filename)  # Load visited URLs\n",
    "\n",
    "    if not visited:\n",
    "        print(f\"Starting from the beginning: {base_url}\")\n",
    "        to_visit = [base_url]\n",
    "    else:\n",
    "        print(f\"Resuming from where left off\")\n",
    "        to_visit = list(visited)\n",
    "        to_visit.remove(load_last_url(csv_filename))  # Remove the last URL since it's already visited\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    while to_visit:\n",
    "        url = to_visit.pop(0)\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "\n",
    "        print(f\"Visiting: {url}\")\n",
    "\n",
    "        try:\n",
    "            html_content = fetch_html(url)\n",
    "        except ConnectionError:\n",
    "            print(\"Connection error, skipping this URL.\")\n",
    "            continue\n",
    "\n",
    "        if html_content:\n",
    "            new_urls = extract_and_filter_urls(html_content, base_url, visited, flexible_target_domain)\n",
    "            to_visit.extend([u for u in new_urls if u not in visited and u not in to_visit])\n",
    "        \n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            print(\"Saving progress...\")\n",
    "            append_to_csv(list(visited), csv_filename)\n",
    "    \n",
    "        sleep(2)\n",
    "\n",
    "    append_to_csv(list(visited), csv_filename)\n",
    "\n",
    "# Existing code below this line\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_filename = '/Users/juanpablocasadobissone/Downloads/tradeurls.csv'\n",
    "    base_url = \"https://www.traderadiators.com/\"\n",
    "    flexible_target_domain = {\"www.traderadiators.com\", \"traderadiators.com\"}  # Add other domains to this set as needed\n",
    "    all_urls = crawl(base_url, csv_filename, flexible_target_domain)\n",
    "    print(\"Crawling complete. URLs saved to 'tradeurls.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02762676-e55c-46c5-9bfb-381e4718fed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site: https://smartlyenergy.com/ with sheet: PageSpeed\n",
      "Read URLs: ['https://smartlyenergy.com/', 'https://smartlyenergy.com/solar-energy/car-parks', 'https://smartlyenergy.com/about-us', 'https://smartlyenergy.com/privacy-policy', 'https://smartlyenergy.com/solar-energy/office-buildings', 'https://smartlyenergy.com/air-source-heat-pumps', 'https://smartlyenergy.com/solar-energy/solar-panels/', 'https://smartlyenergy.com/solar-energy/solar-panels/commercial', 'https://smartlyenergy.com/solar-energy/solar-tiles', 'https://smartlyenergy.com/terms-conditions']\n",
      "Total URLs to process: 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 150\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal URLs to process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_urls\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Fetch detailed PageSpeed Insights using API\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     general_scores, audit_data_for_export \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_page_speed_insights\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAPI_KEY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Check if the API call was successful\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m general_scores \u001b[38;5;129;01mand\u001b[39;00m audit_data_for_export:\n\u001b[1;32m    154\u001b[0m         \n\u001b[1;32m    155\u001b[0m         \u001b[38;5;66;03m# Print the scores for debugging\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m, in \u001b[0;36mfetch_page_speed_insights\u001b[0;34m(url, api_key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_page_speed_insights\u001b[39m(url, api_key):\n\u001b[1;32m     56\u001b[0m     api_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/pagespeedonline/v5/runPagespeed?url=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&key=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&category=performance&category=accessibility&category=best-practices&category=seo&category=pwa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 57\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     59\u001b[0m         detailed_insights \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Pagespeed script:\n",
    "\n",
    "import time \n",
    "from google.oauth2.service_account import Credentials\n",
    "import googleapiclient.discovery\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "\n",
    "# Initialize Google Sheets API\n",
    "def init_sheets_api():  # Added 'def' keyword here\n",
    "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "    SERVICE_ACCOUNT_FILE = '/Users/juanpablocasadobissone/Downloads/boyd-digital-scripts-50288f948562.json'\n",
    "    \n",
    "    credentials = None\n",
    "    credentials = Credentials.from_service_account_file(\n",
    "            SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    \n",
    "    service = googleapiclient.discovery.build('sheets', 'v4', credentials=credentials)\n",
    "    \n",
    "    return service\n",
    "\n",
    "# Function to export data to Google Sheets\n",
    "def export_to_google_sheets(service, sheet_id, general_scores, audit_data_for_export, url, sheet_name=\"Sheet1\"):\n",
    "    sheet = service.spreadsheets()\n",
    "    \n",
    "    values = [[url, \"General Scores\", \"Performance\", \"\", general_scores['performance'], \"\"]]\n",
    "\n",
    "        \n",
    "    for audit in audit_data_for_export:\n",
    "        values = [[url, general_scores['performance'], general_scores['accessibility'], general_scores['best-practices'], general_scores['seo'], general_scores['pwa']]]\n",
    "        body = {'values': [values]}\n",
    "        sheet.values().append(\n",
    "            spreadsheetId=sheet_id,\n",
    "            range=f\"{sheet_name}!A1\",\n",
    "            body=body,\n",
    "            valueInputOption=\"RAW\"\n",
    "        ).execute()\n",
    "        \n",
    "        time.sleep(1) \n",
    "\n",
    "\n",
    "# Function to read URLs from CSV\n",
    "def read_urls_from_csv(csv_file_path):\n",
    "    urls = []\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for index, row in enumerate(csvreader):\n",
    "            if index == 0:  # Skip header row\n",
    "                continue\n",
    "            urls.append(row[0])\n",
    "    return urls\n",
    "\n",
    "# Function to get detailed PageSpeed Insights\n",
    "def fetch_page_speed_insights(url, api_key):\n",
    "    api_url = f\"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&key={api_key}&category=performance&category=accessibility&category=best-practices&category=seo&category=pwa\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        detailed_insights = response.json()\n",
    "\n",
    "        # Capture general scores\n",
    "        categories = detailed_insights.get('lighthouseResult', {}).get('categories', {})\n",
    "        general_scores = {\n",
    "            'performance': categories.get('performance', {}).get('score', 'N/A'),\n",
    "            'accessibility': categories.get('accessibility', {}).get('score', 'N/A'),\n",
    "            'best-practices': categories.get('best-practices', {}).get('score', 'N/A'),\n",
    "            'seo': categories.get('seo', {}).get('score', 'N/A'),\n",
    "            'pwa': categories.get('pwa', {}).get('score', 'N/A')\n",
    "        }\n",
    "\n",
    "        # Capture detailed audits\n",
    "        audits = detailed_insights.get('lighthouseResult', {}).get('audits', {})\n",
    "        audit_data_for_export = []\n",
    "        for audit_name, audit_data in audits.items():\n",
    "            audit_title = audit_data.get('title', 'N/A')\n",
    "            audit_description = audit_data.get('description', 'N/A')\n",
    "            audit_score = audit_data.get('score', 'N/A')\n",
    "            \n",
    "            # Gather resources to optimize\n",
    "            details = audit_data.get('details', {})\n",
    "            items = details.get('items', [])\n",
    "            resources_to_optimize = [item.get('url', 'Unknown') for item in items if isinstance(item, dict)]\n",
    "    \n",
    "            audit_data_for_export.append({\n",
    "                'name': audit_name,\n",
    "                'title': audit_title,\n",
    "                'description': audit_description,\n",
    "                'score': audit_score,\n",
    "                'resources_to_optimize': \", \".join(resources_to_optimize)\n",
    "            })\n",
    "\n",
    "        return general_scores, audit_data_for_export\n",
    "    else:\n",
    "        print(f\"Failed to fetch PageSpeed Insights for URL {url}.\")\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        print(f\"Response Text: {response.text}\")\n",
    "        return None, None  # Return None for both general_scores and audit_data_for_export\n",
    "\n",
    "\n",
    "# Your API Key from Google Cloud Console\n",
    "API_KEY = \"AIzaSyCy1wpV4gHcon4r9hGEK9SYaFWyU9821VU\"\n",
    "\n",
    "# Initialize Google Sheets API\n",
    "service = init_sheets_api()\n",
    "\n",
    "# Add the header row only once\n",
    "SHEET_ID = '1KcgbHy37CBq9-WEiecA5Jq-JjfTxkwV-d-SnDc0jBQQ'\n",
    "header_values = [[\"URL\", \"Audit Name\", \"Title\", \"Description\", \"Score\", \"Resources to Optimize\"]]\n",
    "header_body = {'values': header_values}\n",
    "service.spreadsheets().values().append(\n",
    "    spreadsheetId=SHEET_ID,\n",
    "    range=\"PageSpeed!A1\",\n",
    "    body=header_body,\n",
    "    valueInputOption=\"RAW\"\n",
    ").execute()\n",
    "\n",
    "\n",
    "\n",
    "# Define sites and their corresponding CSV files\n",
    "sites_and_files = [\n",
    "    (\"https://smartlyenergy.com/\", \"/Users/juanpablocasadobissone/Downloads/smartly-filtered-urls.csv\", \"PageSpeed\"),\n",
    "]\n",
    "\n",
    "# Add the header row only once to each sheet\n",
    "SHEET_ID = '1adxRdkjhWduf0BDB-2Vup1CBBUt1nPeajCBn0_N6a3M'\n",
    "header_values = [[\"URL\", \"Performance\", \"Accessibility\", \"Best Practices\", \"SEO\", \"PWA\"]]\n",
    "header_body = {'values': header_values}\n",
    "\n",
    "# Add header to each sheet\n",
    "for _, _, sheet_name in sites_and_files:\n",
    "    service.spreadsheets().values().append(\n",
    "        spreadsheetId=SHEET_ID,\n",
    "        range=f\"{sheet_name}!A:A\",  # Changed to A:A to append at the end\n",
    "        body=header_body,\n",
    "        valueInputOption=\"RAW\"\n",
    "    ).execute()\n",
    "\n",
    "# Main loop to process each site and its corresponding URLs\n",
    "for site, csv_file, sheet_name in sites_and_files:\n",
    "    print(f\"Processing site: {site} with sheet: {sheet_name}\")\n",
    "\n",
    "    urls = read_urls_from_csv(csv_file)[:10]  # Read URLs and limit to first 10 for testing\n",
    "    print(f\"Read URLs: {urls}\")  # Debugging line\n",
    "\n",
    "    total_urls = len(urls)\n",
    "    print(f\"Total URLs to process: {total_urls}\")\n",
    "    \n",
    "    for url in urls:\n",
    "        # Fetch detailed PageSpeed Insights using API\n",
    "        general_scores, audit_data_for_export = fetch_page_speed_insights(url, API_KEY)\n",
    "        \n",
    "        # Check if the API call was successful\n",
    "        if general_scores and audit_data_for_export:\n",
    "            \n",
    "            # Print the scores for debugging\n",
    "            print(f\"Performance Score: {general_scores['performance']}\")\n",
    "            print(f\"Accessibility Score: {general_scores['accessibility']}\")\n",
    "            print(f\"Best Practices Score: {general_scores['best-practices']}\")\n",
    "            print(f\"SEO Score: {general_scores['seo']}\")\n",
    "            print(f\"PWA Score: {general_scores['pwa']}\")\n",
    "            \n",
    "            # Export to Google Sheets\n",
    "            SHEET_ID = '1adxRdkjhWduf0BDB-2Vup1CBBUt1nPeajCBn0_N6a3M'\n",
    "            export_to_google_sheets(service, SHEET_ID, general_scores, audit_data_for_export, url, sheet_name=sheet_name)\n",
    "        \n",
    "        else:\n",
    "            print(\"Failed to fetch PageSpeed Insights.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe40b9c7-1139-4c2b-94ce-19ad65c63eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Starting processing site: https://smartlyenergy.com/ with sheet: PageSpeed ----\n",
      "Total URLs to process: 36\n",
      "Processing URL 83 out of 77: https://smartlyenergy.com/blog/are-solar-panel-finance-options-available\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/are-solar-panel-finance-options-available.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/are-solar-panel-finance-options-available to Google Sheets.\n",
      "Processing URL 84 out of 77: https://smartlyenergy.com/blog/ev-charging-point-installation-cost\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/ev-charging-point-installation-cost.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/ev-charging-point-installation-cost to Google Sheets.\n",
      "Processing URL 85 out of 77: https://smartlyenergy.com/blog/solar-panels-and-shading/\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/solar-panels-and-shading/.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/solar-panels-and-shading/ to Google Sheets.\n",
      "Processing URL 86 out of 77: https://smartlyenergy.com/blog/do-electric-cars-come-with-charging-cables\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/do-electric-cars-come-with-charging-cables.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/do-electric-cars-come-with-charging-cables to Google Sheets.\n",
      "Processing URL 87 out of 77: https://smartlyenergy.com/blog/are-there-government-grants-for-car-charging-points-for-businesses\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/are-there-government-grants-for-car-charging-points-for-businesses.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/are-there-government-grants-for-car-charging-points-for-businesses to Google Sheets.\n",
      "Processing URL 88 out of 77: https://smartlyenergy.com/blog/tesla-solar-roof-tiles\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/tesla-solar-roof-tiles.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/tesla-solar-roof-tiles to Google Sheets.\n",
      "Processing URL 89 out of 77: https://smartlyenergy.com/blog/high-voltage-or-low-voltage-batteries-a-guide\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/high-voltage-or-low-voltage-batteries-a-guide.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/high-voltage-or-low-voltage-batteries-a-guide to Google Sheets.\n",
      "Processing URL 90 out of 77: https://smartlyenergy.com/blog/hies-consumer-code-smartly-energy\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/hies-consumer-code-smartly-energy.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/hies-consumer-code-smartly-energy to Google Sheets.\n",
      "Processing URL 91 out of 77: https://smartlyenergy.com/blog/how-do-solar-farms-work\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/how-do-solar-farms-work.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/how-do-solar-farms-work to Google Sheets.\n",
      "Processing URL 92 out of 77: https://smartlyenergy.com/blog/understanding-energy-efficiency-standards-uk\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/understanding-energy-efficiency-standards-uk.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/understanding-energy-efficiency-standards-uk to Google Sheets.\n",
      "Processing URL 93 out of 77: https://smartlyenergy.com/blog/a-guide-to-solar-panel-mounting-options\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/a-guide-to-solar-panel-mounting-options.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/a-guide-to-solar-panel-mounting-options to Google Sheets.\n",
      "Processing URL 94 out of 77: https://smartlyenergy.com/blog/the-ultimate-ev-charging-cable-and-connector-guide\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/the-ultimate-ev-charging-cable-and-connector-guide.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/the-ultimate-ev-charging-cable-and-connector-guide to Google Sheets.\n",
      "Processing URL 95 out of 77: https://smartlyenergy.com/blog/the-history-of-solar-energy\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/the-history-of-solar-energy.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/the-history-of-solar-energy to Google Sheets.\n",
      "Processing URL 96 out of 77: https://smartlyenergy.com/blog/do-solar-panels-work-at-night\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/do-solar-panels-work-at-night.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/do-solar-panels-work-at-night to Google Sheets.\n",
      "Processing URL 97 out of 77: https://smartlyenergy.com/blog/5-uk-companies-that-are-solar-powered\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/5-uk-companies-that-are-solar-powered.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/5-uk-companies-that-are-solar-powered to Google Sheets.\n",
      "Processing URL 98 out of 77: https://smartlyenergy.com/blog/what-is-eps-for-solar-batteries\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/what-is-eps-for-solar-batteries.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/what-is-eps-for-solar-batteries to Google Sheets.\n",
      "Processing URL 99 out of 77: https://smartlyenergy.com/blog/net-zero-what-does-it-mean-and-how-can-we-achieve-it\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/net-zero-what-does-it-mean-and-how-can-we-achieve-it.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/net-zero-what-does-it-mean-and-how-can-we-achieve-it to Google Sheets.\n",
      "Processing URL 100 out of 77: https://smartlyenergy.com/blog/10-ways-to-reduce-your-energy-bills\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/10-ways-to-reduce-your-energy-bills.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/10-ways-to-reduce-your-energy-bills to Google Sheets.\n",
      "Processing URL 101 out of 77: https://smartlyenergy.com/blog/does-solar-add-value-to-your-property\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/does-solar-add-value-to-your-property.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/does-solar-add-value-to-your-property to Google Sheets.\n",
      "Processing URL 102 out of 77: https://smartlyenergy.com/blog/the-top-5-benefits-of-solar-battery-storage/\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/the-top-5-benefits-of-solar-battery-storage/.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/the-top-5-benefits-of-solar-battery-storage/ to Google Sheets.\n",
      "Processing URL 103 out of 77: https://smartlyenergy.com/blog/what-are-the-benefits-of-solar-panels\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/what-are-the-benefits-of-solar-panels.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/what-are-the-benefits-of-solar-panels to Google Sheets.\n",
      "Processing URL 104 out of 77: https://smartlyenergy.com/blog/do-you-need-planning-permission-for-solar-panels\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/do-you-need-planning-permission-for-solar-panels.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/do-you-need-planning-permission-for-solar-panels to Google Sheets.\n",
      "Processing URL 105 out of 77: https://smartlyenergy.com/blog/the-top-5-benefits-of-solar-battery-storage\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/the-top-5-benefits-of-solar-battery-storage.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/the-top-5-benefits-of-solar-battery-storage to Google Sheets.\n",
      "Processing URL 106 out of 77: https://smartlyenergy.com/blog/what-conditions-are-required-for-optimal-solar-panel-efficiency\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/what-conditions-are-required-for-optimal-solar-panel-efficiency.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/what-conditions-are-required-for-optimal-solar-panel-efficiency to Google Sheets.\n",
      "Processing URL 107 out of 77: https://smartlyenergy.com/blog/ac-coupled-vs-dc-coupled-solar-batteries\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/ac-coupled-vs-dc-coupled-solar-batteries.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/ac-coupled-vs-dc-coupled-solar-batteries to Google Sheets.\n",
      "Processing URL 108 out of 77: https://smartlyenergy.com/blog/solar-inverters-hybrid-inverters-ac-coupled-inverters/\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/solar-inverters-hybrid-inverters-ac-coupled-inverters/.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/solar-inverters-hybrid-inverters-ac-coupled-inverters/ to Google Sheets.\n",
      "Processing URL 109 out of 77: https://smartlyenergy.com/blog/ev-charging-guide\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/ev-charging-guide.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/ev-charging-guide to Google Sheets.\n",
      "Processing URL 110 out of 77: https://smartlyenergy.com/blog/what-is-solar-power\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/what-is-solar-power.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/what-is-solar-power to Google Sheets.\n",
      "Processing URL 111 out of 77: https://smartlyenergy.com/blog/different-types-of-renewable-energy\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/different-types-of-renewable-energy.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/different-types-of-renewable-energy to Google Sheets.\n",
      "Processing URL 112 out of 77: https://smartlyenergy.com/blog/what-are-solar-thermal-panels\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/what-are-solar-thermal-panels.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/what-are-solar-thermal-panels to Google Sheets.\n",
      "Processing URL 113 out of 77: https://smartlyenergy.com/blog/solar-inverters-hybrid-inverters-ac-coupled-inverters\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/solar-inverters-hybrid-inverters-ac-coupled-inverters.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/solar-inverters-hybrid-inverters-ac-coupled-inverters to Google Sheets.\n",
      "Processing URL 114 out of 77: https://smartlyenergy.com/blog/cleaning-solar-panels\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/cleaning-solar-panels.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/cleaning-solar-panels to Google Sheets.\n",
      "Processing URL 115 out of 77: https://smartlyenergy.com/blog/bi-facial-solar-panels-what-are-they-and-how-do-they-work\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/bi-facial-solar-panels-what-are-they-and-how-do-they-work.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/bi-facial-solar-panels-what-are-they-and-how-do-they-work to Google Sheets.\n",
      "Processing URL 116 out of 77: https://smartlyenergy.com/blog/what-is-a-solar-carport\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/what-is-a-solar-carport.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/what-is-a-solar-carport to Google Sheets.\n",
      "Processing URL 117 out of 77: https://smartlyenergy.com/blog/solar-panels-and-shading\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/solar-panels-and-shading.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/solar-panels-and-shading to Google Sheets.\n",
      "Processing URL 118 out of 77: https://smartlyenergy.com/blog/are-solar-panels-on-new-builds-a-must\n",
      "Successfully fetched PageSpeed Insights for URL https://smartlyenergy.com/blog/are-solar-panels-on-new-builds-a-must.\n",
      "Successfully exported data for URL https://smartlyenergy.com/blog/are-solar-panels-on-new-builds-a-must to Google Sheets.\n",
      "---- Finished processing site: https://smartlyenergy.com/ ----\n"
     ]
    }
   ],
   "source": [
    "from google.oauth2.service_account import Credentials\n",
    "import googleapiclient.discovery\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "def get_last_url_in_sheet(service, sheet_id, sheet_name=\"PageSpeed\"):\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().get(spreadsheetId=sheet_id, range=f\"{sheet_name}!A:A\").execute()\n",
    "    values = result.get('values', [])\n",
    "    if values:\n",
    "        return values[-1][0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Initialize Google Sheets API\n",
    "def init_sheets_api():\n",
    "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "    SERVICE_ACCOUNT_FILE = '/Users/juanpablocasadobissone/Downloads/boyd-digital-scripts-50288f948562.json'\n",
    "    \n",
    "    credentials = Credentials.from_service_account_file(\n",
    "            SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    \n",
    "    service = googleapiclient.discovery.build('sheets', 'v4', credentials=credentials)\n",
    "    \n",
    "    return service\n",
    "\n",
    "def init_google_sheet_headers(service, sheet_id, sheet_name=\"PageSpeed\"):\n",
    "    headers = [[\"Url\", \"Performance Score\", \"Accessibility Score\", \"Best Practices Score\", \"SEO Score\", \"PWA Score\"]]\n",
    "    body = {'values': headers}\n",
    "    sheet = service.spreadsheets()\n",
    "    sheet.values().append(\n",
    "        spreadsheetId=sheet_id,\n",
    "        range=f\"{sheet_name}!A1\",\n",
    "        body=body,\n",
    "        valueInputOption=\"RAW\"\n",
    "    ).execute()\n",
    "    \n",
    "    time.sleep(1) \n",
    "\n",
    "# Function to export data to Google Sheets\n",
    "def export_to_google_sheets(service, sheet_id, general_scores, url, sheet_name=\"PageSpeed\"):\n",
    "    sheet = service.spreadsheets()\n",
    "    \n",
    "    values = [[url, general_scores['performance'], general_scores['accessibility'], general_scores['best-practices'], general_scores['seo'], general_scores['pwa']]]\n",
    "    body = {'values': values}\n",
    "    sheet.values().append(\n",
    "        spreadsheetId=sheet_id,\n",
    "        range=f\"{sheet_name}!A1\",\n",
    "        body=body,\n",
    "        valueInputOption=\"RAW\"\n",
    "    ).execute()\n",
    "\n",
    "    time.sleep(1) \n",
    "\n",
    "# Function to read URLs from CSV\n",
    "def read_urls_from_csv(csv_file_path):\n",
    "    urls = []\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for index, row in enumerate(csvreader):\n",
    "            if index == 0:  # Skip header row\n",
    "                continue\n",
    "            urls.append(row[0])\n",
    "    return urls\n",
    "\n",
    "# Function to get detailed PageSpeed Insights\n",
    "def fetch_page_speed_insights(url, api_key):\n",
    "    api_url = f\"https://www.googleapis.com/pagespeedonline/v5/runPagespeed?url={url}&key={api_key}&category=performance&category=accessibility&category=best-practices&category=seo&category=pwa\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        detailed_insights = response.json()\n",
    "\n",
    "        categories = detailed_insights.get('lighthouseResult', {}).get('categories', {})\n",
    "        general_scores = {\n",
    "            'performance': categories.get('performance', {}).get('score', 'N/A'),\n",
    "            'accessibility': categories.get('accessibility', {}).get('score', 'N/A'),\n",
    "            'best-practices': categories.get('best-practices', {}).get('score', 'N/A'),\n",
    "            'seo': categories.get('seo', {}).get('score', 'N/A'),\n",
    "            'pwa': categories.get('pwa', {}).get('score', 'N/A')\n",
    "        }\n",
    "\n",
    "        return general_scores\n",
    "    else:\n",
    "        print(f\"Failed to fetch PageSpeed Insights for URL {url}.\")\n",
    "        return None\n",
    "\n",
    "# Your API Key from Google Cloud Console\n",
    "API_KEY = \"AIzaSyCy1wpV4gHcon4r9hGEK9SYaFWyU9821VU\"\n",
    "\n",
    "# Initialize Google Sheets API\n",
    "service = init_sheets_api()\n",
    "\n",
    "# Define sites and their corresponding CSV files\n",
    "sites_and_files = [\n",
    "    (\"https://smartlyenergy.com/\", \"/Users/juanpablocasadobissone/Downloads/smartly-filtered-urls.csv\", \"PageSpeed\"),\n",
    "]\n",
    "\n",
    "# Initialize Google Sheet headers only if sheet is empty\n",
    "SHEET_ID = '1KcgbHy37CBq9-WEiecA5Jq-JjfTxkwV-d-SnDc0jBQQ'\n",
    "last_url = get_last_url_in_sheet(service, SHEET_ID)  # Fetch the last URL\n",
    "\n",
    "# Only write headers if sheet is empty (last_url is None)\n",
    "if last_url is None:\n",
    "    init_google_sheet_headers(service, SHEET_ID)\n",
    "\n",
    "# Main loop to process each site and its corresponding URLs\n",
    "for site, csv_file, sheet_name in sites_and_files:\n",
    "    print(f\"---- Starting processing site: {site} with sheet: {sheet_name} ----\")\n",
    "\n",
    "    urls = read_urls_from_csv(csv_file)\n",
    "    last_url = get_last_url_in_sheet(service, SHEET_ID, sheet_name=sheet_name)\n",
    "\n",
    "    if last_url and last_url in urls:\n",
    "        start_index = urls.index(last_url) + 1  # Start from the next URL\n",
    "        urls = urls[start_index:]  # Update the list to start from the next URL\n",
    "    else:\n",
    "        start_index = 0  # Start from the beginning if the last URL is not found\n",
    "\n",
    "    total_urls = len(urls)  # Update total_urls to reflect the remaining URLs to process\n",
    "    print(f\"Total URLs to process: {total_urls}\")\n",
    "\n",
    "    for index, url in enumerate(urls, start=start_index):\n",
    "        # The rest of your code here\n",
    "        print(f\"Processing URL {start_index + index + 1} out of {len(urls) + start_index}: {url}\")\n",
    "\n",
    "        general_scores = fetch_page_speed_insights(url, API_KEY)\n",
    "\n",
    "        if general_scores:\n",
    "            print(f\"Successfully fetched PageSpeed Insights for URL {url}.\")\n",
    "            \n",
    "            # Export to Google Sheets\n",
    "            SHEET_ID = '1KcgbHy37CBq9-WEiecA5Jq-JjfTxkwV-d-SnDc0jBQQ'\n",
    "            export_to_google_sheets(service, SHEET_ID, general_scores, url, sheet_name=sheet_name)\n",
    "            print(f\"Successfully exported data for URL {url} to Google Sheets.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch or export data for URL {url}.\")\n",
    "    \n",
    "    print(f\"---- Finished processing site: {site} ----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e88cd9d-d85b-449b-84c9-1869a786dba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL: https://smartlyenergy.com/privacy-policy\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/solar-energy/solar-panels/\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/terms-conditions\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/contact-us/\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/sustainability/\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/solar-energy/leisure-centres\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/terms-of-website-use\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/solar-energy/roofit-solar\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/cookie-policy\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/what-are-terracotta-solar-roof-tiles\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/air-source-heat-pumps-vs-ground-source-heat-pumps-whats-the-difference\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/air-source-heat-pumps\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/how-much-do-solar-panels-save\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/renting-your-ev-charger-peer-to-peer-charging\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/can-i-install-solar-panels-on-listed-buildings\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/ev-charging-point-installation-cost\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/solar-panels-and-shading/\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/do-electric-cars-come-with-charging-cables\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/are-there-government-grants-for-car-charging-points-for-businesses\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/high-voltage-or-low-voltage-batteries-a-guide\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/understanding-energy-efficiency-standards-uk\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/the-history-of-solar-energy\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/do-solar-panels-work-at-night\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/5-uk-companies-that-are-solar-powered\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/net-zero-what-does-it-mean-and-how-can-we-achieve-it\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/10-ways-to-reduce-your-energy-bills\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/the-top-5-benefits-of-solar-battery-storage/\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/what-are-the-benefits-of-solar-panels\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/do-you-need-planning-permission-for-solar-panels\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/what-conditions-are-required-for-optimal-solar-panel-efficiency\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/ac-coupled-vs-dc-coupled-solar-batteries\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/solar-inverters-hybrid-inverters-ac-coupled-inverters/\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/what-is-solar-power\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/different-types-of-renewable-energy\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/what-are-solar-thermal-panels\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/cleaning-solar-panels\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/bi-facial-solar-panels-what-are-they-and-how-do-they-work\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/solar-panels-and-shading\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n",
      "Processing URL: https://smartlyenergy.com/blog/are-solar-panels-on-new-builds-a-must\n",
      "Error: No data returned. Full response: {'responseAggregationType': 'byPage'}\n"
     ]
    }
   ],
   "source": [
    "#GSC Script\n",
    "\n",
    "from google.oauth2.service_account import Credentials\n",
    "import googleapiclient.discovery\n",
    "import datetime\n",
    "from urllib.parse import quote\n",
    "import csv\n",
    "import time  # Import the time module if you haven't already\n",
    "\n",
    "\n",
    "# Read URLs from CSV\n",
    "with open('/Users/juanpablocasadobissone/Downloads/smartly-filtered-urls.csv', 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    urls = [row[0] for row in csvreader]\n",
    "\n",
    "# Add this after reading `urls` from your source CSV\n",
    "urls_to_process = [url for url in urls if url not in existing_urls]\n",
    "\n",
    "\n",
    "# Initialize credentials with additional scope for Google Sheets\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    '/Users/juanpablocasadobissone/Downloads/boyd-digital-scripts-50288f948562.json', \n",
    "    scopes=[\n",
    "        'https://www.googleapis.com/auth/webmasters.readonly',\n",
    "        'https://www.googleapis.com/auth/spreadsheets'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build the service\n",
    "webmasters_service = googleapiclient.discovery.build('webmasters', 'v3', credentials=credentials)\n",
    "\n",
    "# Build the Google Sheets service\n",
    "sheets_service = googleapiclient.discovery.build('sheets', 'v4', credentials=credentials)\n",
    "\n",
    "# Add this after initializing your sheets_service\n",
    "existing_sheet_data = sheets_service.spreadsheets().values().get(\n",
    "    spreadsheetId=sheet_id,\n",
    "    range=f\"{sheet_name}!A2:A\"  # Assuming column A contains URLs, starting from row 2\n",
    ").execute()\n",
    "\n",
    "existing_urls = [item[0] for item in existing_sheet_data.get(\"values\", [])]\n",
    "\n",
    "\n",
    "# Initialize dates and URLs\n",
    "start_date = datetime.date.today() - datetime.timedelta(days=90)\n",
    "end_date = datetime.date.today() - datetime.timedelta(days=1)\n",
    "site_url = \"https://smartlyenergy.com\"  # Note the sc-domain: prefix\n",
    "\n",
    "# Initialize last_row before loop\n",
    "last_row = 1  # Start from the first row\n",
    "\n",
    "# Define these before using them\n",
    "sheet_id = '1KcgbHy37CBq9-WEiecA5Jq-JjfTxkwV-d-SnDc0jBQQ'\n",
    "sheet_name = 'GSC Data'\n",
    "\n",
    "# Add headers to Google Sheet\n",
    "sheets_service.spreadsheets().values().update(\n",
    "    spreadsheetId=sheet_id,\n",
    "    range=f\"{sheet_name}!A1:F1\",\n",
    "    body={\n",
    "        \"values\": [[\"URL\", \"Query\", \"Position\", \"Impressions\", \"Clicks\", \"CTR\"]]\n",
    "    },\n",
    "    valueInputOption=\"RAW\"\n",
    ").execute()\n",
    "\n",
    "# Loop through URLs\n",
    "for target_url in urls_to_process:\n",
    "    # Create payload\n",
    "    payload = {\n",
    "        \"startDate\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"endDate\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"dimensions\": [\"query\", \"page\"],\n",
    "        \"rowLimit\": 5000,\n",
    "        \"fields\": \"position,impressions,clicks,ctr\",\n",
    "        \"dimensionFilterGroups\": [\n",
    "            {\n",
    "                \"filters\": [\n",
    "                    {\n",
    "                        \"dimension\": \"page\",\n",
    "                        \"expression\": target_url,\n",
    "                        \"operator\": \"EQUALS\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"dimension\": \"country\",\n",
    "                        \"expression\": \"GBR\",\n",
    "                        \"operator\": \"EQUALS\"\n",
    "                    }\n",
    "                ],\n",
    "                \"groupType\": \"AND\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create payload with current target_url\n",
    "    payload['dimensionFilterGroups'][0]['filters'][0]['expression'] = target_url\n",
    "\n",
    "    # Make the API call\n",
    "    response = webmasters_service.searchanalytics().query(siteUrl=site_url, body=payload).execute()\n",
    "\n",
    "    # Initialize a dictionary to store queries for each URL\n",
    "    url_query_data = {}\n",
    "\n",
    "    # Check the response and add data to Google Sheet\n",
    "    if 'rows' in response:        \n",
    "        # Group queries by URL and sort them by clicks\n",
    "        for row in response['rows']:\n",
    "            query, url = row['keys']\n",
    "            if url not in url_query_data:\n",
    "                url_query_data[url] = []\n",
    "            url_query_data[url].append(row)\n",
    "            \n",
    "        for url, queries in url_query_data.items():\n",
    "            url_query_data[url] = sorted(queries, key=lambda x: x['impressions'], reverse=True)[:10]\n",
    "\n",
    "        current_sheet_data = []  # Do not include headers\n",
    "\n",
    "        # Prepare data for Google Sheets\n",
    "        for url, queries in url_query_data.items():\n",
    "            for query_data in queries:\n",
    "                current_sheet_data.append([\n",
    "                    url,\n",
    "                    query_data['keys'][0],  # Query\n",
    "                    query_data['position'],\n",
    "                    query_data['impressions'],\n",
    "                    query_data['clicks'],\n",
    "                    query_data['ctr']\n",
    "                ])\n",
    "    \n",
    "        # Update Google Sheet\n",
    "        range_name = f\"{sheet_name}!A{last_row + 1}:F{last_row + len(current_sheet_data)}\"\n",
    "\n",
    "        # Update last_row\n",
    "        last_row += len(current_sheet_data)\n",
    "        \n",
    "        # Add this block to update the Google Sheet with new data\n",
    "        sheets_service.spreadsheets().values().update(\n",
    "            spreadsheetId=sheet_id,\n",
    "            range=range_name,\n",
    "            body={\n",
    "                \"values\": current_sheet_data\n",
    "            },\n",
    "            valueInputOption=\"RAW\"\n",
    "        ).execute()\n",
    "        \n",
    "        time.sleep(1)  # Pause for 1 second\n",
    "    else:\n",
    "        print(f\"Error: No data returned. Full response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19059235-47b9-439d-96d6-6e62865aee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the analysis...\n",
      "Analysis complete for https://smartlyenergy.com/.\n",
      "Analysis complete for https://smartlyenergy.com/solar-energy/car-parks.\n",
      "Analysis complete for https://smartlyenergy.com/about-us.\n",
      "Analysis complete for https://smartlyenergy.com/solar-energy/office-buildings.\n",
      "Analysis complete for https://smartlyenergy.com/air-source-heat-pumps.\n",
      "Analysis finished.\n",
      "Results exported to CSV.\n"
     ]
    }
   ],
   "source": [
    "#Doubles and triples\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = '/Users/juanpablocasadobissone/Downloads/Smartly - GSC Data.csv' \n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "# Organize the data by URL and associated queries\n",
    "url_to_queries = {}\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    query = row['Query']\n",
    "    impressions = row['Impressions']\n",
    "    if url in url_to_queries:\n",
    "        url_to_queries[url].append((query, impressions))\n",
    "    else:\n",
    "        url_to_queries[url] = [(query, impressions)]\n",
    "\n",
    "# Sort queries by impressions and take top 20\n",
    "for url, queries in url_to_queries.items():\n",
    "    sorted_queries = sorted(queries, key=lambda x: x[1], reverse=True)[:20]\n",
    "    url_to_queries[url] = [query for query, _ in sorted_queries]\n",
    "\n",
    "# A function to fetch the content of a URL and return the text\n",
    "def fetch_page_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            return ' '.join(soup.stripped_strings)  # Concatenate all strings, separated by spaces\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# A function to analyze the page content based on the associated queries\n",
    "def analyze_content(url, queries):\n",
    "    content = fetch_page_content(url)\n",
    "    if content is None:\n",
    "        return None\n",
    "    \n",
    "    missing_singles = []\n",
    "    missing_doubles = []\n",
    "    missing_triplets = []\n",
    "    \n",
    "    for query in queries:\n",
    "        words = query.split()\n",
    "        \n",
    "        # Check for missing single words\n",
    "        for word in words:\n",
    "            if word.lower() not in content.lower():\n",
    "                missing_singles.append(word)\n",
    "        \n",
    "        # Check for missing double words\n",
    "        for i in range(len(words) - 1):\n",
    "            double_word = f\"{words[i]} {words[i + 1]}\"\n",
    "            if double_word.lower() not in content.lower():\n",
    "                missing_doubles.append(double_word)\n",
    "                \n",
    "        # Check for missing triplets\n",
    "        for i in range(len(words) - 2):\n",
    "            triplet_word = f\"{words[i]} {words[i + 1]} {words[i + 2]}\"\n",
    "            if triplet_word.lower() not in content.lower():\n",
    "                missing_triplets.append(triplet_word)\n",
    "                \n",
    "    # Remove duplicates\n",
    "    missing_singles = list(set(missing_singles))\n",
    "    missing_doubles = list(set(missing_doubles))\n",
    "    missing_triplets = list(set(missing_triplets))\n",
    "    \n",
    "    return sorted(set(missing_singles))[:20], sorted(set(missing_doubles))[:20], sorted(set(missing_triplets))[:20]\n",
    "\n",
    "# Take a sample of the first 5 URLs for demonstration\n",
    "sample_5_urls = {key: url_to_queries[key] for key in list(url_to_queries)[:5]}\n",
    "\n",
    "# Create an empty list to hold the analysis results\n",
    "final_results = []\n",
    "\n",
    "# Analyze each sample URL\n",
    "print(\"Starting the analysis...\")\n",
    "for url, queries in sample_5_urls.items():\n",
    "    result = analyze_content(url, queries)\n",
    "    if result is not None:\n",
    "        # Unpack the result into missing_singles, missing_doubles, and missing_triplets\n",
    "        missing_singles, missing_doubles, missing_triplets = result\n",
    "        # Add the associated queries as a comma-separated string\n",
    "        associated_queries = \", \".join(queries)\n",
    "        # Append the analysis results to the final_results list\n",
    "        final_results.append([url, associated_queries, \", \".join(missing_singles), \", \".join(missing_doubles), \", \".join(missing_triplets)])\n",
    "        print(f\"Analysis complete for {url}.\")\n",
    "    else:\n",
    "        print(f\"Skipping {url} due to fetch or analysis issues.\")\n",
    "\n",
    "print(\"Analysis finished.\")\n",
    "\n",
    "# Export the final_results to a CSV\n",
    "final_df = pd.DataFrame(final_results, columns=['URL', 'Queries', 'Missing Singles', 'Missing Doubles', 'Missing Triplets']) \n",
    "final_df.to_csv('/Users/juanpablocasadobissone/Downloads/finalgssmartly.csv', index=False)  \n",
    "\n",
    "print(\"Results exported to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a66a5b-cecb-4467-a486-fb247c5d353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Starting loop to process URLs...\n",
      "Processing URL 1 of 5\n",
      "Scraping content from https://www.traderadiators.com/blog...\n",
      "Content successfully scraped with class: content\n",
      "Calling OpenAI API...\n",
      "Received response from OpenAI API.\n",
      "Length of new_sentences: 1\n",
      "Processing URL 2 of 5\n",
      "Scraping content from https://www.traderadiators.com/radiators/vertical-radiators...\n",
      "Content successfully scraped with class: content\n",
      "Calling OpenAI API...\n",
      "Received response from OpenAI API.\n",
      "Length of new_sentences: 2\n",
      "Processing URL 3 of 5\n",
      "Scraping content from https://www.traderadiators.com/radiators...\n",
      "Content successfully scraped with class: content\n",
      "Calling OpenAI API...\n",
      "Received response from OpenAI API.\n",
      "Length of new_sentences: 3\n",
      "Processing URL 4 of 5\n",
      "Scraping content from https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators...\n",
      "Content successfully scraped with class: content\n",
      "Calling OpenAI API...\n",
      "Received response from OpenAI API.\n",
      "Length of new_sentences: 4\n",
      "Processing URL 5 of 5\n",
      "Scraping content from https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators...\n",
      "Content successfully scraped with class: content\n",
      "Calling OpenAI API...\n",
      "Received response from OpenAI API.\n",
      "Length of new_sentences: 5\n",
      "Adding new sentences to DataFrame...\n",
      "Saving DataFrame to new CSV file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from selenium.webdriver.common.keys import Keys  # Make sure to add this import at the top of your script\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Read the finalgsclast.csv\n",
    "print(\"Reading CSV file...\")  # New print statement\n",
    "df = pd.read_csv('/Users/juanpablocasadobissone/Downloads/finalgstraderadiators.csv')\n",
    "\n",
    "\n",
    "# Function to scrape content using Selenium and BeautifulSoup\n",
    "def scrape_content(url):\n",
    "    try:\n",
    "        print(f\"Scraping content from {url}...\")\n",
    "        \n",
    "        # Initialize the Chrome Service\n",
    "        service = Service(executable_path='/Users/juanpablocasadobissone/Downloads/chromedriver-mac-arm64 2/chromedriver')\n",
    "        \n",
    "        # Create Chrome options and set to headless\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize ChromeDriver\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        # Navigate to the URL\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Get the HTML content of the page\n",
    "        html_content = driver.page_source\n",
    "        \n",
    "        # Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # List of possible classes\n",
    "        possible_classes = ['content', 'entry-content', 'elementor-column elementor-col-50 elementor-top-column elementor-element elementor-element-cd4d9ce']\n",
    "\n",
    "        # Try to find the div using possible classes\n",
    "        content_div = None\n",
    "        for class_name in possible_classes:\n",
    "            content_div = soup.find('div', {'class': class_name})\n",
    "            if content_div:\n",
    "                break\n",
    "    \n",
    "        if content_div:\n",
    "            # Get only the text of the first three <p> elements\n",
    "            p_tags = content_div.find_all('p', limit=3)\n",
    "            truncated_content = \" \".join([p.text for p in p_tags])\n",
    "            \n",
    "            print(f\"Content successfully scraped with class: {class_name}\")\n",
    "            return truncated_content.strip()  # Return the concatenated and stripped text\n",
    "        else:\n",
    "            print(\"Content not found.\")\n",
    "            return \"Content not found\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping: {e}\")  # Existing print statement\n",
    "        return str(e)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def call_openai(missing_doubles, missing_triplets, content):\n",
    "    try:\n",
    "        print(\"Calling OpenAI API...\")\n",
    "        system_msg = \"You are a helpful assistant that generates new sentences for articles based on missing keywords.\"\n",
    "        user_msg = f\"Generate 5 new sentences for the article based on these missing doubles: {missing_doubles} and missing triplets: {missing_triplets}. The article's current content is: {content}\"\n",
    "        \n",
    "        # Count the total tokens\n",
    "        total_tokens = len(tokenizer.tokenize(user_msg))  # Replace with OpenAI's token counter if available\n",
    "        if total_tokens > 4097:\n",
    "            raise ValueError(f\"Total tokens exceed the OpenAI API limit: {total_tokens} > 4097\")\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\", \"content\": user_msg}\n",
    "            ]\n",
    "        )\n",
    "        print(\"Received response from OpenAI API.\")\n",
    "        return response['choices'][0]['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while calling OpenAI API: {e}\")\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "# Your existing function to summarize text\n",
    "def summarize_text(text, max_tokens=1024):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "    # Truncate the text to max_tokens if it exceeds the limit\n",
    "    if len(tokenized_text) > max_tokens:\n",
    "        tokenized_text = tokenized_text[:max_tokens]\n",
    "        text = tokenizer.convert_tokens_to_string(tokenized_text)\n",
    "\n",
    "    inputs = tokenizer([text], max_length=max_tokens, return_tensors=\"pt\", truncation=True)\n",
    "    summary_ids = model.generate(inputs.input_ids, num_beams=4, min_length=30, max_length=100, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "    \n",
    "    # Initialize a list to store the individual summaries\n",
    "    summary_list = []\n",
    "    \n",
    "    # Loop to handle long text\n",
    "    for i in range(0, len(tokenized_text), max_chunk_size):\n",
    "        # Get a chunk of tokens\n",
    "        chunk = tokenized_text[i:i + max_chunk_size]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "\n",
    "        # Prepare the inputs\n",
    "        inputs = tokenizer([chunk_text], max_length=max_chunk_size, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "        # Generate the summary IDs\n",
    "        summary_ids = model.generate(inputs.input_ids, num_beams=4, min_length=30, max_length=100, early_stopping=True)\n",
    "        \n",
    "        # Decode and store the summary\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summary_list.append(summary)\n",
    "    \n",
    "    # Concatenate the individual summaries to get the final summary\n",
    "    final_summary = ' '.join(summary_list)\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# Initialize an empty list to store new sentences\n",
    "new_sentences = []\n",
    "\n",
    "# Loop through each row and generate new sentences\n",
    "print(\"Starting loop to process URLs...\")\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Processing URL {index+1} of {len(df)}\")\n",
    "    try:\n",
    "        # Your existing code\n",
    "        url = row['URL']\n",
    "        missing_doubles = row['Missing Doubles']\n",
    "        missing_triplets = row['Missing Triplets']\n",
    "\n",
    "        # Scrape content using Selenium and BeautifulSoup\n",
    "        content = scrape_content(url)\n",
    "\n",
    "        # Tokenize and truncate the content right here\n",
    "        tokenized_content = tokenizer.tokenize(content)\n",
    "        if len(tokenized_content) > 1024:\n",
    "            tokenized_content = tokenized_content[:1024]\n",
    "            content = tokenizer.convert_tokens_to_string(tokenized_content)\n",
    "\n",
    "        # Summarize the content if it's too long\n",
    "        summarized_content = summarize_text(content)\n",
    "\n",
    "        # Generate 10 new sentences using OpenAI API\n",
    "        generated_sentences = call_openai(missing_doubles, missing_triplets, summarized_content)\n",
    "        \n",
    "        new_sentences.append(generated_sentences)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred at index {index}: {e}\")\n",
    "        new_sentences.append(\"Error occurred\")  # Add a placeholder\n",
    "\n",
    "    print(f\"Length of new_sentences: {len(new_sentences)}\")\n",
    "\n",
    "# This goes outside your loop\n",
    "try:\n",
    "    print(\"Adding new sentences to DataFrame...\")\n",
    "    df['New Sentences'] = new_sentences\n",
    "except ValueError as ve:\n",
    "    print(f\"ValueError: {ve}\")\n",
    "\n",
    "# Your existing code to save the DataFrame\n",
    "print(\"Saving DataFrame to new CSV file...\")\n",
    "df.to_csv('/Users/juanpablocasadobissone/Downloads/trade_with_new_sentences.csv', index=False)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4ba4a5-c7c7-4f26-80cb-3265c4c13dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com\n",
      "\n",
      "Processing 2 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/checkout/cart/\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/checkout/cart/\n",
      "\n",
      "Processing 3 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators\n",
      "\n",
      "Processing 4 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators\n",
      "\n",
      "Processing 5 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators\n",
      "\n",
      "Processing 6 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/checkout/\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/checkout/\n",
      "\n",
      "Processing 7 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators\n",
      "\n",
      "Processing 8 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/heating-calculator\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/heating-calculator\n",
      "\n",
      "Processing 9 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/column-radiators/2-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/column-radiators/2-column-radiators\n",
      "\n",
      "Processing 10 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/blog\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/blog\n",
      "\n",
      "Processing 11 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/column-radiators/4-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/column-radiators/4-column-radiators\n",
      "\n",
      "Processing 12 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/column-radiators/modern-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/column-radiators/modern-column-radiators\n",
      "\n",
      "Processing 13 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/flat-panel-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/flat-panel-vertical-radiators\n",
      "\n",
      "Processing 14 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/checkout/\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/checkout/\n",
      "\n",
      "Processing 15 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/column-radiators/with-feet-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/column-radiators/with-feet-column-radiators\n",
      "\n",
      "Processing 16 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/double-panel-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/double-panel-vertical-radiators\n",
      "\n",
      "Processing 17 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/\n",
      "\n",
      "Processing 18 of 18 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/checkout/cart/\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "Extracting heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/checkout/cart/\n",
      "\n",
      "All URLs processed. Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "#Technical Data\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Function to scrape webpage content\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Function to extract and count internal and external links\n",
    "def extract_links(soup, domain):\n",
    "    internal_links = []\n",
    "    external_links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = a_tag['href']\n",
    "        if domain in link:\n",
    "            internal_links.append(link)\n",
    "        else:\n",
    "            external_links.append(link)\n",
    "    return len(internal_links), len(external_links)\n",
    "\n",
    "# Function to extract and count internal and external links\n",
    "def extract_links(soup, domain):\n",
    "    internal_links = []\n",
    "    external_links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = a_tag['href']\n",
    "        if domain in link:\n",
    "            internal_links.append(link)\n",
    "        else:\n",
    "            external_links.append(link)\n",
    "    return internal_links, external_links  # Now returning the actual URLs\n",
    "\n",
    "# Function to extract meta title\n",
    "def extract_meta_title(soup):\n",
    "    title_tag = soup.find('title')\n",
    "    title = title_tag.string if title_tag else \"N/A\"\n",
    "    return title, len(title)\n",
    "\n",
    "# Function to extract meta tags\n",
    "def extract_meta_tags(soup):\n",
    "    meta_desc_tag = soup.find('meta', {'name': 'description'})\n",
    "    meta_desc = meta_desc_tag['content'] if meta_desc_tag else \"N/A\"\n",
    "    return meta_desc, len(meta_desc)\n",
    "\n",
    "# Function to extract heading tags\n",
    "def extract_headings(soup):\n",
    "    h1 = [tag.text for tag in soup.find_all('h1')]\n",
    "    h2 = [tag.text for tag in soup.find_all('h2')]\n",
    "    h3 = [tag.text for tag in soup.find_all('h3')]\n",
    "    return h1, h2, h3\n",
    "\n",
    "# Function to get article length\n",
    "def get_article_length(soup):\n",
    "    article_text = soup.find('body').text  # This might need to be more specific depending on the webpage structure\n",
    "    return len(article_text.split())\n",
    "\n",
    "# Main script\n",
    "input_csv = \"/Users/juanpablocasadobissone/Downloads/tradesample3.csv\"  # Replace this with your input CSV file containing URLs\n",
    "output_csv = \"/Users/juanpablocasadobissone/Downloads/tradetechnical.csv\"  # The output CSV file to store the analysis\n",
    "\n",
    "# Read URLs from input CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Prepare the output CSV\n",
    "with open(output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"URL\", \"Internal Links\", \"External Links\", \"Meta Description\", \"Meta Description Length\", \"H1\", \"H2\", \"H3\", \"Article Length\"])\n",
    "\n",
    "    # Loop through each URL and perform the analysis\n",
    "    total_urls = len(df)\n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"Processing {index + 1} of {total_urls} URLs...\")  # Progress indicator\n",
    "        \n",
    "        url = row.iloc[0]\n",
    "        print(f\"Analyzing URL: {url}\")  # Display the URL being analyzed\n",
    "        \n",
    "        domain = url.split(\"//\")[-1].split(\"/\")[0]\n",
    "        \n",
    "        # Scrape page\n",
    "        print(\"Scraping page...\")\n",
    "        soup = scrape_page(url)\n",
    "        \n",
    "        # Extract and count links\n",
    "        print(\"Extracting internal and external links...\")\n",
    "        internal_links, external_links = extract_links(soup, domain)\n",
    "        \n",
    "        # Extract meta tags\n",
    "        print(\"Extracting meta tags...\")\n",
    "        meta_desc, meta_desc_length = extract_meta_tags(soup)\n",
    "        \n",
    "        # Extract heading tags\n",
    "        print(\"Extracting heading tags...\")\n",
    "        h1, h2, h3 = extract_headings(soup)\n",
    "        \n",
    "        # Get article length\n",
    "        print(\"Calculating article length...\")\n",
    "        article_length = get_article_length(soup)\n",
    "        \n",
    "        # Write to the output CSV\n",
    "        print(\"Writing data to output CSV...\")\n",
    "        writer.writerow([url, internal_links, external_links, meta_desc, meta_desc_length, h1, h2, h3, article_length])\n",
    "\n",
    "        print(f\"Completed analysis for {url}\\n\")  # Completion message for each URL\n",
    "\n",
    "print(\"All URLs processed. Analysis complete.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "509bbde3-c397-40d9-b082-e8a41813ecf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: the radiator centre for URL: https://www.traderadiators.com/blog\n",
      "Found competitors for query 'the radiator centre': ['https://www.theradiatorcentre.com/', 'https://www.contourheating.co.uk/blog/long-lst-radiators-last#:~:text=The%20average%20life%20of%20a,and%20care%20you%20put%20in.', 'https://en.wikipedia.org/wiki/Radiator_(heating)#:~:text=A%20hot%2Dwater%20radiator%20consists,pipe%20at%20the%20other%20end.']\n",
      "Processing query: the radiator centre for URL: https://www.traderadiators.com/blog\n",
      "Found competitors for query 'the radiator centre': ['https://www.theradiatorcentre.com/', 'https://www.contourheating.co.uk/blog/long-lst-radiators-last#:~:text=The%20average%20life%20of%20a,and%20care%20you%20put%20in.', \"https://www.heatandplumb.com/blog/how-often-should-radiators-be-replaced#:~:text=If%20you're%20looking%20for,between%20fifteen%20and%20twenty%20years.\"]\n",
      "Processing query: radiator blog for URL: https://www.traderadiators.com/blog\n",
      "Found competitors for query 'radiator blog': ['https://www.blog.radiator.debacle.us/', 'https://radiatorsonline.com/blog', 'https://hudevad.com/en/blog/']\n",
      "Processing query: btu calculator for URL: https://www.traderadiators.com/heating-calculator\n",
      "Found competitors for query 'btu calculator': ['https://www.calculator.net/btu-calculator.html', 'https://www.pioneerminisplit.com/blogs/news/how-many-btus-you-need-for-your-room-or-house#:~:text=Generally%2C%20your%20temperature%20control%20system,capability%20of%20around%208%2C000%20BTU.', 'https://www.thespruce.com/air-conditioning-chart-1152654']\n",
      "Processing query: btu calculator uk for URL: https://www.traderadiators.com/heating-calculator\n",
      "Found competitors for query 'btu calculator uk': ['https://radiatorsonline.com/btu-calculator', 'https://www.diy.com/ideas-advice/calculators/btu-radiator-calculator', 'https://ukradiators.com/pages/btu-calculator']\n",
      "Processing query: radiator calculator for URL: https://www.traderadiators.com/heating-calculator\n",
      "Found competitors for query 'radiator calculator': ['https://www.bestheating.com/btu-calculator', \"https://www.bestheating.com/btu-calculator#:~:text=%3E%3E%20How%20is%20your%20room's%20BTU,below%20and%20besides%20the%20room.\", 'https://webcache.googleusercontent.com/search?q=cache:xALQtEuhxScJ:https://www.bestheating.com/btu-calculator&hl=en&gl=us']\n",
      "Processing query: radiators for URL: https://www.traderadiators.com/radiators\n",
      "Found competitors for query 'radiators': ['https://en.wikipedia.org/wiki/Radiator', 'https://www.bewarmer.co.uk/7-reasons-why-radiators-are-a-bad-way-to-heat-your-home/#:~:text=Radiators%20are%20inefficient,efficient%20compared%20with%20surface%20heating.', 'https://www.kia.com/dm/discover-kia/ask/what-is-a-radiator-in-a-car.html#:~:text=A%20radiator%20is%20the%20key,the%20rest%20of%20the%20engine.']\n",
      "Processing query: radiator for URL: https://www.traderadiators.com/radiators\n",
      "Found competitors for query 'radiator': ['https://www.autozone.com/cooling-heating-and-climate-control/radiator', 'https://www.kia.com/dm/discover-kia/ask/what-is-a-radiator-in-a-car.html#:~:text=A%20radiator%20is%20the%20key,the%20rest%20of%20the%20engine.', 'https://webcache.googleusercontent.com/search?q=cache:OW92EYtMYJoJ:https://www.kia.com/dm/discover-kia/ask/what-is-a-radiator-in-a-car.html&hl=en&gl=us']\n",
      "Processing query: radiators uk for URL: https://www.traderadiators.com/radiators\n",
      "Found competitors for query 'radiators uk': ['https://ukradiators.com/', 'https://www.screwfix.com/c/heating-plumbing/radiators/cat830960', 'https://www.bestheating.com/radiators.html']\n",
      "Processing query: column radiators for URL: https://www.traderadiators.com/radiators/column-radiators/2-column-radiators\n",
      "Found competitors for query 'column radiators': ['https://www.screwfix.com/c/heating-plumbing/column-radiators/cat5840006', 'https://www.designerradiatorsdirect.co.uk/blog/what-is-a-column-radiator-and-are-they-efficient#:~:text=Column%20radiators%20have%20been%20a,of%20class%20to%20your%20room.', 'https://ukradiators.com/blogs/buying-guides/is-a-column-radiator-right-for-you#:~:text=One%20of%20the%20most%20well,to%20suit%20your%20heating%20needs.']\n",
      "Processing query: column radiator for URL: https://www.traderadiators.com/radiators/column-radiators/2-column-radiators\n",
      "Found competitors for query 'column radiator': ['https://www.screwfix.com/c/heating-plumbing/column-radiators/cat5840006', 'https://www.designerradiatorsdirect.co.uk/blog/what-is-a-column-radiator-and-are-they-efficient#:~:text=Column%20radiators%20have%20been%20a,of%20class%20to%20your%20room.', 'https://www.bestheating.com/info/are-column-radiators-more-efficient/#:~:text=What%20is%20a%20column%20radiator,body%20that%20heats%20up%20faster.']\n",
      "Processing query: column radiators uk for URL: https://www.traderadiators.com/radiators/column-radiators/2-column-radiators\n",
      "Found competitors for query 'column radiators uk': ['https://www.traderadiators.com/radiators/column-radiators', 'https://ukradiators.com/collections/column-radiators', 'https://www.columnrads.co.uk/']\n",
      "Processing query: column radiators for URL: https://www.traderadiators.com/radiators/column-radiators/modern-column-radiators\n",
      "Found competitors for query 'column radiators': ['https://www.traderadiators.com/radiators/column-radiators', 'https://www.designerradiatorsdirect.co.uk/blog/what-is-a-column-radiator-and-are-they-efficient#:~:text=Column%20radiators%20have%20been%20a,of%20class%20to%20your%20room.', 'https://ukradiators.com/blogs/buying-guides/is-a-column-radiator-right-for-you#:~:text=One%20of%20the%20most%20well,to%20suit%20your%20heating%20needs.']\n",
      "Processing query: column radiator for URL: https://www.traderadiators.com/radiators/column-radiators/modern-column-radiators\n",
      "Found competitors for query 'column radiator': ['https://usa.hudsonreed.com/designer-radiators/cast-iron-style-radiators', 'https://www.designerradiatorsdirect.co.uk/blog/what-is-a-column-radiator-and-are-they-efficient#:~:text=Column%20radiators%20have%20been%20a,of%20class%20to%20your%20room.', 'https://www.bestheating.com/info/are-column-radiators-more-efficient/#:~:text=What%20is%20a%20column%20radiator,body%20that%20heats%20up%20faster.']\n",
      "Processing query: column radiators uk for URL: https://www.traderadiators.com/radiators/column-radiators/modern-column-radiators\n",
      "Found competitors for query 'column radiators uk': ['https://www.traderadiators.com/radiators/column-radiators', 'https://ukradiators.com/collections/column-radiators', 'https://www.columnrads.co.uk/']\n",
      "Processing query: column radiators with feet for URL: https://www.traderadiators.com/radiators/column-radiators/with-feet-column-radiators\n",
      "Found competitors for query 'column radiators with feet': ['https://www.designerradiatorshowroom.co.uk/blog/expert-guide-to-column-radiators-what-you-should-know/#:~:text=Most%20solid%20walls%20will%20be,feet%20to%20handle%20the%20weight.', 'https://www.traderadiators.com/accessories/radiator-foot-supports#:~:text=Radiators%20can%20be%20large%20and,in%20position%20for%20the%20future.', 'https://webcache.googleusercontent.com/search?q=cache:HJv_zZ5eo2MJ:https://www.traderadiators.com/accessories/radiator-foot-supports&hl=en&gl=us']\n",
      "Processing query: vertical radiators for URL: https://www.traderadiators.com/radiators/vertical-radiators\n",
      "Found competitors for query 'vertical radiators': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://elegant-radiators.co.uk/blogs/news/pros-and-cons-vertical-radiators']\n",
      "Processing query: tall radiators for URL: https://www.traderadiators.com/radiators/vertical-radiators\n",
      "Found competitors for query 'tall radiators': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://featureradiators.co.uk/blog/post/the-benefits-of-vertical-radiators/']\n",
      "Processing query: vertical radiator for URL: https://www.traderadiators.com/radiators/vertical-radiators\n",
      "Found competitors for query 'vertical radiator': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://elegant-radiators.co.uk/blogs/news/pros-and-cons-vertical-radiators']\n",
      "Processing query: vertical radiators for URL: https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators\n",
      "Found competitors for query 'vertical radiators': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://usa.hudsonreed.com/designer-radiators/rubi-radiators', 'https://runtalnorthamerica.com/residential-hydronic-vertical-panels/']\n",
      "Processing query: tall radiators for URL: https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators\n",
      "Found competitors for query 'tall radiators': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://featureradiators.co.uk/blog/post/the-benefits-of-vertical-radiators/']\n",
      "Processing query: vertical radiator for URL: https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators\n",
      "Found competitors for query 'vertical radiator': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://elegant-radiators.co.uk/blogs/news/pros-and-cons-vertical-radiators']\n",
      "Processing query: vertical radiator for URL: https://www.traderadiators.com/radiators/vertical-radiators/double-panel-vertical-radiators\n",
      "Found competitors for query 'vertical radiator': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://elegant-radiators.co.uk/blogs/news/pros-and-cons-vertical-radiators']\n",
      "Processing query: tall radiators for URL: https://www.traderadiators.com/radiators/vertical-radiators/double-panel-vertical-radiators\n",
      "Found competitors for query 'tall radiators': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://featureradiators.co.uk/blog/post/the-benefits-of-vertical-radiators/']\n",
      "Processing query: wall radiators for URL: https://www.traderadiators.com/radiators/vertical-radiators/double-panel-vertical-radiators\n",
      "Found competitors for query 'wall radiators': ['https://usa.hudsonreed.com/designer-radiators', 'https://www.victorianplumbing.co.uk/bathroom-ideas-and-inspiration/are-vertical-radiators-efficient#:~:text=So%20when%20it%20comes%20to,by%20BTU%20than%20anything%20else.', 'https://elegant-radiators.co.uk/blogs/news/pros-and-cons-vertical-radiators']\n",
      "Processing query: vertical radiators for URL: https://www.traderadiators.com/radiators/vertical-radiators/flat-panel-vertical-radiators\n",
      "Found competitors for query 'vertical radiators': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://elegant-radiators.co.uk/blogs/news/pros-and-cons-vertical-radiators']\n",
      "Processing query: tall radiators for URL: https://www.traderadiators.com/radiators/vertical-radiators/flat-panel-vertical-radiators\n",
      "Found competitors for query 'tall radiators': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://featureradiators.co.uk/blog/post/the-benefits-of-vertical-radiators/']\n",
      "Processing query: vertical radiator for URL: https://www.traderadiators.com/radiators/vertical-radiators/flat-panel-vertical-radiators\n",
      "Found competitors for query 'vertical radiator': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://elegant-radiators.co.uk/blogs/news/pros-and-cons-vertical-radiators']\n",
      "Processing query: vertical radiators for URL: https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators\n",
      "Found competitors for query 'vertical radiators': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://elegant-radiators.co.uk/blogs/news/pros-and-cons-vertical-radiators']\n",
      "Processing query: column radiators for URL: https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators\n",
      "Found competitors for query 'column radiators': ['https://www.traderadiators.com/radiators/column-radiators', 'https://www.designerradiatorsdirect.co.uk/blog/what-is-a-column-radiator-and-are-they-efficient#:~:text=Column%20radiators%20have%20been%20a,of%20class%20to%20your%20room.', 'https://ukradiators.com/blogs/buying-guides/is-a-column-radiator-right-for-you#:~:text=One%20of%20the%20most%20well,to%20suit%20your%20heating%20needs.']\n",
      "Processing query: tall radiators for URL: https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators\n",
      "Found competitors for query 'tall radiators': ['https://usa.hudsonreed.com/designer-radiators/vertical-radiators', 'https://ukradiators.com/blogs/buying-guides/pros-and-cons-of-a-vertical-radiator#:~:text=Vertical%20Radiators%20are%20just%20as%20efficient%20as%20other%20designs&text=This%20is%20the%20same%20for,well%20as%20a%20horizontal%20one.', 'https://featureradiators.co.uk/blog/post/the-benefits-of-vertical-radiators/']\n",
      "Processing 1 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/blog\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/blog\n",
      "\n",
      "Processing 2 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/blog\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/blog\n",
      "\n",
      "Processing 3 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/blog\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/blog\n",
      "\n",
      "Processing 4 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/blog\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/blog\n",
      "\n",
      "Processing 5 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/blog\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/blog\n",
      "\n",
      "Processing 6 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/blog\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/blog\n",
      "\n",
      "Processing 7 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/heating-calculator\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/heating-calculator\n",
      "\n",
      "Processing 8 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/heating-calculator\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/heating-calculator\n",
      "\n",
      "Processing 9 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/heating-calculator\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/heating-calculator\n",
      "\n",
      "Processing 10 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators\n",
      "\n",
      "Processing 11 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators\n",
      "\n",
      "Processing 12 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators\n",
      "\n",
      "Processing 13 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/column-radiators/2-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/column-radiators/2-column-radiators\n",
      "\n",
      "Processing 14 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/column-radiators/2-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/column-radiators/2-column-radiators\n",
      "\n",
      "Processing 15 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/column-radiators/2-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/column-radiators/2-column-radiators\n",
      "\n",
      "Processing 16 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/column-radiators/modern-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/column-radiators/modern-column-radiators\n",
      "\n",
      "Processing 17 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/column-radiators/modern-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/column-radiators/modern-column-radiators\n",
      "\n",
      "Processing 18 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/column-radiators/modern-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/column-radiators/modern-column-radiators\n",
      "\n",
      "Processing 19 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/column-radiators/with-feet-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/column-radiators/with-feet-column-radiators\n",
      "\n",
      "Processing 20 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators\n",
      "\n",
      "Processing 21 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators\n",
      "\n",
      "Processing 22 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators\n",
      "\n",
      "Processing 23 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators\n",
      "\n",
      "Processing 24 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators\n",
      "\n",
      "Processing 25 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/designer-vertical-radiators\n",
      "\n",
      "Processing 26 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/double-panel-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/double-panel-vertical-radiators\n",
      "\n",
      "Processing 27 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/double-panel-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/double-panel-vertical-radiators\n",
      "\n",
      "Processing 28 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/double-panel-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/double-panel-vertical-radiators\n",
      "\n",
      "Processing 29 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/flat-panel-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/flat-panel-vertical-radiators\n",
      "\n",
      "Processing 30 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/flat-panel-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/flat-panel-vertical-radiators\n",
      "\n",
      "Processing 31 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/flat-panel-vertical-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/flat-panel-vertical-radiators\n",
      "\n",
      "Processing 32 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators\n",
      "\n",
      "Processing 33 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators\n",
      "\n",
      "Processing 34 of 34 URLs...\n",
      "Analyzing URL: https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators\n",
      "Scraping page...\n",
      "Extracting internal and external links...\n",
      "Extracting meta tags...\n",
      "Extracting meta title...\n",
      "x heading tags...\n",
      "Calculating article length...\n",
      "Writing data to output CSV...\n",
      "Completed analysis for https://www.traderadiators.com/radiators/vertical-radiators/vertical-column-radiators\n",
      "\n",
      "All URLs processed. Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "#Competitors script\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# Create a new instance of Options\n",
    "chrome_options = Options()\n",
    "\n",
    "# Use the --lang=en-GB flag to set the language to English (United Kingdom)\n",
    "chrome_options.add_argument(\"--lang=en-GB\")\n",
    "\n",
    "# Use the --geoip-country=GB flag to set the geo-location to United Kingdom\n",
    "chrome_options.add_argument(\"--geoip-country=GB\")\n",
    "\n",
    "# Initialize the Chrome Service\n",
    "service = Service(executable_path='/Users/juanpablocasadobissone/Downloads/chromedriver-mac-arm64 2/chromedriver')\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "input_csv = \"/Users/juanpablocasadobissone/Downloads/querydata.csv\"\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Group by 'URL' and get the top 3 queries for each URL based on 'Impressions'\n",
    "top_queries = df.groupby('URL').apply(lambda x: x.nlargest(3, 'Impressions')['Query']).reset_index(level=1, drop=True)\n",
    "\n",
    "# Prepare the output CSV for competitor information\n",
    "competitor_output_csv = \"/Users/juanpablocasadobissone/Downloads/querydatafinal.csv\"\n",
    "\n",
    "with open(competitor_output_csv, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"URL\", \"Query\", \"Competitor 1\", \"Competitor 2\", \"Competitor 3\"])\n",
    "\n",
    "    # Loop through each URL and its top queries\n",
    "    for url, queries in top_queries.groupby(level=0):\n",
    "        for query in queries:\n",
    "            print(f\"Processing query: {query} for URL: {url}\")\n",
    "            \n",
    "            # Initialize Chrome options for headless mode\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "            \n",
    "            # Use Selenium to search Google and find the top 3 competitors\n",
    "            with webdriver.Chrome(service=service, options=chrome_options) as driver:\n",
    "                driver.get(f\"https://www.google.com/search?q={query}\")\n",
    "                \n",
    "                # Extract competitor URLs (this part may vary based on Google's page structure)\n",
    "                competitor_elements = driver.find_elements(By.CSS_SELECTOR, \".yuRUbf a\")[:3]\n",
    "                competitors = [elem.get_attribute('href') for elem in competitor_elements]\n",
    "                \n",
    "            # Write to the output CSV\n",
    "            writer.writerow([url, query] + competitors)\n",
    "\n",
    "            print(f\"Found competitors for query '{query}': {competitors}\")\n",
    "\n",
    "\n",
    "# Read competitor URLs from the competitor_output_csv into a new DataFrame\n",
    "competitor_output_csv = \"/Users/juanpablocasadobissone/Downloads/querydatafinal.csv\"  \n",
    "df_competitors = pd.read_csv(competitor_output_csv)\n",
    "df_competitors = pd.merge(df_competitors, df[['URL', 'Query', 'Impressions']], on=['URL', 'Query'], how='left')\n",
    "\n",
    "\n",
    "#BeautifulSoup code to get SEO metrics and write them to a different CSV\n",
    "seo_output_csv = \"/Users/juanpablocasadobissone/Downloads/competitortechnical.csv\"\n",
    "\n",
    "# Function to scrape webpage content\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Function to extract and count internal and external links\n",
    "def extract_links(soup, domain):\n",
    "    internal_links = []\n",
    "    external_links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        link = a_tag['href']\n",
    "        if domain in link:\n",
    "            internal_links.append(link)\n",
    "        else:\n",
    "            external_links.append(link)\n",
    "    return internal_links, external_links  \n",
    "\n",
    "# Function to extract meta title\n",
    "def extract_meta_title(soup):\n",
    "    title_tag = soup.find('title')\n",
    "    title = title_tag.string if title_tag else \"N/A\"\n",
    "    return title, len(title)\n",
    "\n",
    "# Function to extract meta tags\n",
    "def extract_meta_tags(soup):\n",
    "    meta_desc_tag = soup.find('meta', {'name': 'description'})\n",
    "    meta_desc = meta_desc_tag['content'] if meta_desc_tag else \"N/A\"\n",
    "    return meta_desc, len(meta_desc)\n",
    "\n",
    "# Function to extract heading tags\n",
    "def extract_headings(soup):\n",
    "    h1 = [tag.text for tag in soup.find_all('h1')]\n",
    "    h2 = [tag.text for tag in soup.find_all('h2')]\n",
    "    h3 = [tag.text for tag in soup.find_all('h3')]\n",
    "    return h1, h2, h3\n",
    "\n",
    "# Function to get article length\n",
    "def get_article_length(soup):\n",
    "    article_text = soup.find('body').text  # This might need to be more specific depending on the webpage structure\n",
    "    return len(article_text.split())\n",
    "\n",
    "# Prepare the output CSV\n",
    "with open(seo_output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"URL\", \"Internal Links\", \"Internal Links Details\", \"External Links\", \"External Links Details\", \"Meta Description\", \"Meta Description Length\", \"Meta Title\", \"Meta Title Length\", \"H1\", \"H2\", \"H3\", \"Article Length\"])\n",
    "\n",
    "    # Loop through each URL and perform the analysis\n",
    "    total_urls = len(df_competitors)\n",
    "    for index, row in df_competitors.iterrows():\n",
    "        # Get the Query and Impressions from df_competitors DataFrame\n",
    "        query = row['Query']\n",
    "        impressions = row['Impressions']\n",
    "        \n",
    "        print(f\"Processing {index + 1} of {total_urls} URLs...\")  # Progress indicator\n",
    "        \n",
    "        url = row.iloc[0]\n",
    "        print(f\"Analyzing URL: {url}\")  # Display the URL being analyzed\n",
    "        \n",
    "        domain = url.split(\"//\")[-1].split(\"/\")[0]\n",
    "        \n",
    "        # Scrape page\n",
    "        print(\"Scraping page...\")\n",
    "        soup = scrape_page(url)\n",
    "\n",
    "        # Extract and count links\n",
    "        print(\"Extracting internal and external links...\")\n",
    "        internal_links, external_links = extract_links(soup, domain)\n",
    "        \n",
    "        # Extract meta tags\n",
    "        print(\"Extracting meta tags...\")\n",
    "        meta_desc, meta_desc_length = extract_meta_tags(soup)\n",
    "        \n",
    "        # Extract meta title\n",
    "        print(\"Extracting meta title...\")\n",
    "        meta_title, meta_title_length = extract_meta_title(soup)\n",
    "        \n",
    "        # Extract heading tags\n",
    "        print(\"x heading tags...\")\n",
    "        h1, h2, h3 = extract_headings(soup)\n",
    "        \n",
    "        # Get article length\n",
    "        print(\"Calculating article length...\")\n",
    "        article_length = get_article_length(soup)\n",
    "        \n",
    "        print(\"Writing data to output CSV...\")\n",
    "        # Fixing the code\n",
    "        # Get the Query and Impressions from df_competitors DataFrame\n",
    "        query = row['Query']\n",
    "        impressions = row['Impressions']\n",
    "        \n",
    "        # Modify the writer.writerow\n",
    "        writer.writerow([url, query, impressions, len(internal_links), internal_links, len(external_links), external_links, meta_desc, meta_desc_length, meta_title, meta_title_length, h1, h2, h3, article_length])\n",
    "\n",
    "        print(f\"Completed analysis for {url}\\n\")  # Completion message for each URL\n",
    "\n",
    "print(\"All URLs processed. Analysis complete.\")  # Final completion message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1432abf0-e8e7-45d5-870f-b3b9fce03ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main function started\n",
      "Fetching the document content...\n",
      "Extracting the entire text from the document...\n",
      "Extending the text to approximately 1000 words...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 298\u001b[0m\n\u001b[1;32m    295\u001b[0m     save_text_to_word(extended_text, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/juanpablocasadobissone/Downloads/Extended_Text.docx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 298\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 234\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m text \u001b[38;5;241m=\u001b[39m extract_text_from_doc(doc)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtending the text to approximately 1000 words...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 234\u001b[0m extended_text \u001b[38;5;241m=\u001b[39m \u001b[43mextend_text_to_approx_1000_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtended Text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextended_text[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print the first 100 characters\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting the title tag...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 145\u001b[0m, in \u001b[0;36mextend_text_to_approx_1000_words\u001b[0;34m(initial_text, target_word_count)\u001b[0m\n\u001b[1;32m    142\u001b[0m extended_text \u001b[38;5;241m=\u001b[39m initial_text  \u001b[38;5;66;03m# Start with the initial text\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m count_words(extended_text) \u001b[38;5;241m<\u001b[39m target_word_count:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# Get the next chunk of text from GPT-3\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdavinci\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# You can adjust this based on how large you want each extension to be\u001b[39;49;00m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     new_text \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Append the new text to the extended_text\u001b[39;00m\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupplied_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[38;5;241m.\u001b[39msession_create_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mabs_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTIMEOUT_SECS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest timed out: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/my_project/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "import textstat\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import openai\n",
    "import datetime\n",
    "import googleapiclient.discovery\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Initialize Google Docs API client\n",
    "credentials = Credentials.from_service_account_file('/Users/juanpablocasadobissone/Downloads/boyd-digital-scripts-50288f948562.json', scopes=['https://www.googleapis.com/auth/documents', 'https://www.googleapis.com/auth/drive'])\n",
    "docs_service = build('docs', 'v1', credentials=credentials)\n",
    "\n",
    "# Initialize SpellChecker\n",
    "spell = SpellChecker()  # Initialize the \n",
    "\n",
    "def save_text_to_word(text, filename):\n",
    "    doc = Document()\n",
    "    doc.add_heading('Generated Text', 0)\n",
    "    \n",
    "    # Add the text\n",
    "    doc.add_paragraph(text)\n",
    "    \n",
    "    # Save the document\n",
    "    doc.save(filename)\n",
    "\n",
    "def identify_important_keywords(text):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in identifying important keywords for SEO.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Identify the most important keyword in the following text: {text}\"}\n",
    "        ],\n",
    "    )\n",
    "    content = response['choices'][0]['message']['content']\n",
    "    \n",
    "    # Extract only the keyword from the sentence\n",
    "    keyword_match = re.search(r'\"(.*?)\"', content)\n",
    "    if keyword_match:\n",
    "        return keyword_match.group(1)\n",
    "    return None\n",
    "\n",
    "def extract_all_text_from_doc(doc):\n",
    "    all_text = \"\"\n",
    "    for content in doc['body']['content']:\n",
    "        if 'paragraph' in content:\n",
    "            for element in content['paragraph']['elements']:\n",
    "                if 'textRun' in element:\n",
    "                    all_text += element['textRun']['content']\n",
    "    print(\"All text elements:\")\n",
    "    print(all_text)\n",
    "\n",
    "def generate_slug(text):\n",
    "    # Remove punctuation and other non-alphabetic characters\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return cleaned_text.lower().replace(' ', '-')\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "    \n",
    "\n",
    "def extract_text_from_doc(doc):\n",
    "    text = \"\"\n",
    "    for content in doc['body']['content']:\n",
    "        if 'paragraph' in content:\n",
    "            for element in content['paragraph']['elements']:\n",
    "                if 'textRun' in element:\n",
    "                    text += element['textRun']['content']\n",
    "    return text\n",
    "\n",
    "def extract_title_from_doc(doc):\n",
    "    all_text = \"\"\n",
    "    for content in doc['body']['content']:\n",
    "        if 'paragraph' in content:\n",
    "            for element in content['paragraph']['elements']:\n",
    "                if 'textRun' in element:\n",
    "                    all_text += element['textRun']['content']\n",
    "                    \n",
    "    title_match = re.search(r'Title tag:([^\\n]*)', all_text)\n",
    "    if title_match:\n",
    "        return title_match.group(1).strip()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "    \n",
    "def get_doc_content(document_id):\n",
    "    try:\n",
    "        doc = docs_service.documents().get(documentId=document_id).execute()\n",
    "        return doc  # Return the entire document dictionary\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return {}\n",
    "\n",
    "def check_spelling(text):\n",
    "    # Remove punctuation and other non-alphabetic characters\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    misspelled = spell.unknown(cleaned_text.split())\n",
    "    return list(misspelled)  # Return only the list of potentially misspelled words\n",
    "\n",
    "def find_tag_content(text, tag):\n",
    "    # Create a regular expression to find content between tags\n",
    "    regex_pattern = f\"{tag}: (.+?)(?:\\n|$)\"\n",
    "    match = re.search(regex_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def calculate_extension_score(text, ideal_min_len, ideal_max_len, main_keyword):\n",
    "    score = 0\n",
    "    if ideal_min_len <= len(text) <= ideal_max_len:\n",
    "        score += 1  # Length is within the ideal range\n",
    "    if main_keyword.lower() in text.lower():\n",
    "        score += 1  # Main keyword is present\n",
    "    return score\n",
    "\n",
    "def calculate_readability(text):\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "def calculate_keyword_density(text, keyword):\n",
    "    total_words = len(text.split())\n",
    "    keyword_count = text.lower().count(keyword.lower())\n",
    "    return (keyword_count / total_words) * 100\n",
    "\n",
    "def calculate_extension_score(content, min_len, max_len, keyword):\n",
    "    score = 0\n",
    "    if content:\n",
    "        if min_len <= len(content) <= max_len:\n",
    "            score += 50\n",
    "        if keyword.lower() in content.lower():\n",
    "            score += 50\n",
    "    return \n",
    "\n",
    "def extend_text_to_approx_1000_words(initial_text, target_word_count=1000):\n",
    "    try:\n",
    "        extended_text = initial_text  # Start with the initial text\n",
    "        while count_words(extended_text) < target_word_count:\n",
    "            # Get the next chunk of text from GPT-3\n",
    "            response = openai.Completion.create(\n",
    "            engine=\"davinci\",\n",
    "            prompt=extended_text,\n",
    "            temperature=0.3,\n",
    "            max_tokens=100,  # You can adjust this based on how large you want each extension to be\n",
    "            top_p=1,\n",
    "            frequency_penalty=0.5,\n",
    "            presence_penalty=0.5,\n",
    "            stop=[\"\\n\"]\n",
    "        )\n",
    "            new_text = response['choices'][0]['text'].strip()\n",
    "            \n",
    "            # Append the new text to the extended_text\n",
    "            extended_text += \" \" + new_text\n",
    "            \n",
    "            # Check if we've reached the target word count\n",
    "            if count_words(extended_text) >= target_word_count:\n",
    "                break\n",
    "    \n",
    "        return extended_text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define fetch_similar_queries function\n",
    "''' def fetch_similar_queries(main_keyword):\n",
    "    credentials = Credentials.from_service_account_file(\n",
    "        '/Users/juanpablocasadobissone/Downloads/boyd-digital-scripts-50288f948562.json',\n",
    "        scopes=[\n",
    "            'https://www.googleapis.com/auth/webmasters.readonly',\n",
    "            'https://www.googleapis.com/auth/spreadsheets'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    webmasters_service = googleapiclient.discovery.build('webmasters', 'v3', credentials=credentials)\n",
    "\n",
    "    start_date = datetime.date.today() - datetime.timedelta(days=90)\n",
    "    end_date = datetime.date.today() - datetime.timedelta(days=1)\n",
    "    site_url = \"sc-domain:traderadiators.com\"\n",
    "\n",
    "    payload = {\n",
    "        \"startDate\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"endDate\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"dimensions\": [\"query\"],\n",
    "        \"rowLimit\": 5000,\n",
    "        \"fields\": \"position,impressions,clicks,ctr\",\n",
    "        \"dimensionFilterGroups\": [\n",
    "            {\n",
    "                \"filters\": [\n",
    "                    {\n",
    "                        \"dimension\": \"query\",\n",
    "                        \"expression\": main_keyword,\n",
    "                        \"operator\": \"CONTAINS\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"dimension\": \"country\",\n",
    "                        \"expression\": \"GBR\",\n",
    "                        \"operator\": \"EQUALS\"\n",
    "                    }\n",
    "                ],\n",
    "                \"groupType\": \"AND\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = webmasters_service.searchanalytics().query(siteUrl=site_url, body=payload).execute()\n",
    "\n",
    "    if 'rows' in response:\n",
    "        top_queries = sorted(response['rows'], key=lambda x: x['impressions'], reverse=True)[:3]\n",
    "        return [(row['keys'][0], row['impressions']) for row in top_queries]\n",
    "    else:\n",
    "        print(f\"Error: No data returned. Full response: {response}\")\n",
    "        return []\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    print(\"Main function started\")\n",
    "    document_id = \"1jReaLd1cs9DDVUhY484PnG48vWGH2vSReq-GlJ3x-fo\"\n",
    "    \n",
    "    print(\"Fetching the document content...\")\n",
    "    doc = get_doc_content(document_id)\n",
    "    if not doc:\n",
    "        print(\"Failed to fetch document. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    print(\"Extracting the entire text from the document...\")\n",
    "    text = extract_text_from_doc(doc)\n",
    "    \n",
    "    print(\"Extending the text to approximately 1000 words...\")\n",
    "    extended_text = extend_text_to_approx_1000_words(text)\n",
    "    \n",
    "    print(f\"Extended Text: {extended_text[:100]}...\")  # Print the first 100 characters\n",
    "    \n",
    "    print(\"Extracting the title tag...\")\n",
    "    title = extract_title_from_doc(doc)\n",
    "    print(f\"Extracted Title Tag: {title}\")\n",
    "\n",
    "    print(\"Identifying main keyword...\")\n",
    "    main_keyword = identify_important_keywords(title)\n",
    "    print(f\"Main Keyword Identified by GPT-3.5: {main_keyword}\")\n",
    "\n",
    "    slug = generate_slug(main_keyword)\n",
    "    print(f\"Generated Slug: {slug}\")\n",
    "\n",
    "    # Find H1 and Meta Description content\n",
    "    h1_content = find_tag_content(text, 'H1')\n",
    "    meta_content = find_tag_content(text, 'Meta Description')\n",
    "    \n",
    "    # Calculate extension scores\n",
    "    h1_score = calculate_extension_score(h1_content, 20, 60, main_keyword) if h1_content else 0\n",
    "    meta_score = calculate_extension_score(meta_content, 50, 160, main_keyword) if meta_content else 0\n",
    "    \n",
    "    # Calculate character count for H1 and Meta Description\n",
    "    h1_char_count = len(h1_content) if h1_content else 0\n",
    "    meta_char_count = len(meta_content) if meta_content else 0\n",
    "    \n",
    "    print(f\"H1 Extension Score: {h1_score}\")\n",
    "    print(f\"Meta Description Extension Score: {meta_score}\")\n",
    "\n",
    "    print(f\"H1 Character Count: {h1_char_count}\")\n",
    "    print(f\"Meta Description Character Count: {meta_char_count}\")\n",
    "    \n",
    "    # Fetch similar queries from Google Search Console\n",
    "    #similar_queries = fetch_similar_queries(main_keyword)\n",
    "    \n",
    "    # Define important_keywords\n",
    "    important_keywords = [main_keyword]\n",
    "\n",
    "    # Check spelling\n",
    "    spelling_errors = check_spelling(text)\n",
    "    print(f\"Spelling Errors: {spelling_errors}\")\n",
    "\n",
    "    # Evaluate length\n",
    "    length_score = len(text.split())\n",
    "    print(f\"Length Score: {length_score}\")\n",
    "\n",
    "    # Evaluate readability\n",
    "    readability_score = calculate_readability(text)\n",
    "    print(f\"Readability Score: {readability_score}\")\n",
    "\n",
    "    # Calculate keyword density for each important keyword\n",
    "    main_keyword = identify_important_keywords(title)\n",
    "    print(f\"Main Keyword Identified by GPT-3.5: {main_keyword}\")\n",
    "    \n",
    "    if main_keyword:  # Check if the main_keyword exists\n",
    "        # Calculate keyword density for the GPT-3.5 identified keyword\n",
    "        density = calculate_keyword_density(text, main_keyword)\n",
    "        print(f\"Keyword density of {main_keyword} is: {density}%\")\n",
    "        \n",
    "    # Save the extended text to a Word document\n",
    "    save_text_to_word(extended_text, '/Users/juanpablocasadobissone/Downloads/Extended_Text.docx')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80c166-506b-4264-bf51-828f1723e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "import re\n",
    "\n",
    "# Initialize Google Docs API client\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    '/Users/juanpablocasadobissone/Downloads/boyd-digital-scripts-50288f948562.json', \n",
    "    scopes=['https://www.googleapis.com/auth/documents', 'https://www.googleapis.com/auth/drive']\n",
    ")\n",
    "docs_service = build('docs', 'v1', credentials=credentials)\n",
    "\n",
    "def extract_title_from_doc(doc):\n",
    "    all_text = \"\"\n",
    "    for content in doc['body']['content']:\n",
    "        if 'paragraph' in content:\n",
    "            for element in content['paragraph']['elements']:\n",
    "                if 'textRun' in element:\n",
    "                    all_text += element['textRun']['content']\n",
    "                    \n",
    "    title_match = re.search(r'Title tag:([^\\n]*)', all_text)\n",
    "    if title_match:\n",
    "        return title_match.group(1).strip()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def extract_all_text_from_doc(doc):\n",
    "    all_text = \"\"\n",
    "    for content in doc['body']['content']:\n",
    "        if 'paragraph' in content:\n",
    "            for element in content['paragraph']['elements']:\n",
    "                if 'textRun' in element:\n",
    "                    all_text += element['textRun']['content']\n",
    "    print(\"All text elements:\")\n",
    "    print(all_text)\n",
    "\n",
    "def get_doc_content(document_id):\n",
    "    try:\n",
    "        doc = docs_service.documents().get(documentId=document_id).execute()\n",
    "        return doc\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return {}\n",
    "\n",
    "def main():\n",
    "    document_id = \"1jReaLd1cs9DDVUhY484PnG48vWGH2vSReq-GlJ3x-fo\"\n",
    "    doc = get_doc_content(document_id)\n",
    "    \n",
    "    title = extract_title_from_doc(doc)\n",
    "    print(f\"Extracted Title Tag: {title}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "766dd3f2-5599-414b-ad0c-b4a64cbf2098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: copper designer radiators\n",
      "Position: 4.169491525423728\n",
      "Impressions: 59\n",
      "Clicks: 0\n",
      "CTR: 0\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "from google.oauth2.service_account import Credentials\n",
    "import googleapiclient.discovery\n",
    "import datetime\n",
    "\n",
    "# Initialize credentials with scope for Google Search Console\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    '/Users/juanpablocasadobissone/Downloads/boyd-digital-scripts-50288f948562.json',\n",
    "    scopes=['https://www.googleapis.com/auth/webmasters.readonly']\n",
    ")\n",
    "\n",
    "# Initialize dates and URLs\n",
    "start_date = datetime.date.today() - datetime.timedelta(days=90)\n",
    "end_date = datetime.date.today() - datetime.timedelta(days=1)\n",
    "site_url = \"sc-domain:traderadiators.com\"  # Note the sc-domain: prefix\n",
    "main_keyword = \"Copper Designer Radiators\"  # The keyword to query for\n",
    "\n",
    "# Build the Google Search Console API client\n",
    "webmasters_service = googleapiclient.discovery.build('webmasters', 'v3', credentials=credentials)\n",
    "\n",
    "# Create payload for the API request\n",
    "payload = {\n",
    "    \"startDate\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "    \"endDate\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "    \"dimensions\": [\"query\"],\n",
    "    \"rowLimit\": 100,\n",
    "    \"fields\": \"position,impressions,clicks,ctr\",\n",
    "    \"dimensionFilterGroups\": [\n",
    "        {\n",
    "            \"filters\": [\n",
    "                {\n",
    "                    \"dimension\": \"query\",\n",
    "                    \"expression\": main_keyword,\n",
    "                    \"operator\": \"CONTAINS\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Make the API call\n",
    "response = webmasters_service.searchanalytics().query(siteUrl=site_url, body=payload).execute()\n",
    "\n",
    "# Print the API response\n",
    "if 'rows' in response:\n",
    "    for row in response['rows']:\n",
    "        print(f\"Query: {row['keys'][0]}\")\n",
    "        print(f\"Position: {row['position']}\")\n",
    "        print(f\"Impressions: {row['impressions']}\")\n",
    "        print(f\"Clicks: {row['clicks']}\")\n",
    "        print(f\"CTR: {row['ctr']}\")\n",
    "        print(\"------\")\n",
    "else:\n",
    "    print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d49823d-b9d5-449b-892a-4fa16450f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import openai\n",
    "import enchant\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from textstat import textstat\n",
    "import string\n",
    "from googleapiclient.errors import HttpError\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "\n",
    "# Set OpenAI API Key and Google API Key\n",
    "load_dotenv(\"google.env\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize Google Docs API client\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    '/Users/juanpablocasadobissone/Downloads/boyd-digital-scripts-50288f948562.json', \n",
    "    scopes=['https://www.googleapis.com/auth/documents', 'https://www.googleapis.com/auth/drive']\n",
    ")\n",
    "docs_service = build('docs', 'v1', credentials=credentials)\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "# Define function to read Google Doc\n",
    "def read_google_doc(document_id):\n",
    "    doc = docs_service.documents().get(documentId=document_id).execute()\n",
    "    full_text = ''\n",
    "    for element in doc['body']['content']:\n",
    "        if 'paragraph' in element:\n",
    "            for run in element['paragraph']['elements']:\n",
    "                if 'textRun' in run:\n",
    "                    full_text += run['textRun']['content']\n",
    "    return full_text\n",
    "\n",
    "# Function to update a Google Doc\n",
    "def update_google_doc(document_id, new_content):\n",
    "    try:\n",
    "        requests = [\n",
    "            {\n",
    "                'deleteContentRange': {\n",
    "                    'range': {\n",
    "                        'startIndex': 1,\n",
    "                        'endIndex': 2  # Adjust according to where you want to stop deleting\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'insertText': {\n",
    "                    'location': {\n",
    "                        'index': 1,\n",
    "                    },\n",
    "                    'text': new_content\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        docs_service.documents().batchUpdate(\n",
    "            documentId=document_id,\n",
    "            body={'requests': requests}\n",
    "        ).execute()\n",
    "\n",
    "        logging.info(f\"Updated Google Doc with ID: {document_id}\")\n",
    "\n",
    "    except HttpError as error:\n",
    "        logging.error(f\"An error occurred: {error}\")\n",
    "\n",
    "# Existing function for identifying important keywords\n",
    "def identify_important_keywords(text):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in identifying important keywords for SEO.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Identify the most important keyword in the following text: {text}\"}\n",
    "        ],\n",
    "    )\n",
    "    content = response['choices'][0]['message']['content']\n",
    "    keyword_match = re.search(r'\"(.*?)\"', content)\n",
    "    # Add this inside identify_important_url after making the GPT-3.5 API call\n",
    "    logging.info(f\"GPT-3.5 Response content: {content}\")\n",
    "    return keyword_match.group(1) if keyword_match else None\n",
    "\n",
    "# UK style GPT check\n",
    "def rephrase_to_professional_uk(text):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant specialized in rephrasing text into professional UK English.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Translate the following text to professional UK English:\\n{text}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "\n",
    "\n",
    "# Function for spell check\n",
    "def spellcheck_uk(text):\n",
    "    dictionary = enchant.Dict(\"en_UK\")\n",
    "    mistakes = []\n",
    "    for word in text.split():\n",
    "        cleaned_word = re.sub(r'[{}]'.format(string.punctuation), '', word)\n",
    "        if cleaned_word and not dictionary.check(cleaned_word):\n",
    "            mistakes.append(cleaned_word)\n",
    "    return mistakes\n",
    "\n",
    "def keyword_density(text, keyword):\n",
    "    # Remove punctuation from text and keyword\n",
    "    cleaned_text = re.sub(r'[{}]'.format(string.punctuation), '', text)\n",
    "    cleaned_keyword = re.sub(r'[{}]'.format(string.punctuation), '', keyword)\n",
    "\n",
    "    total_words = len(cleaned_text.split())\n",
    "    keyword_occurrences = cleaned_text.lower().count(cleaned_keyword.lower())\n",
    "    \n",
    "    return (keyword_occurrences / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "# Function for US to UK terms\n",
    "def translate_to_uk(text):\n",
    "    us_to_uk_dict = {'color': 'colour', 'analyze': 'analyse'}  # Add more\n",
    "    for us, uk in us_to_uk_dict.items():\n",
    "        text = text.replace(us, uk)\n",
    "    return text\n",
    "\n",
    "# Function to create and populate a Google Doc\n",
    "def create_google_doc(title, content):\n",
    "    try:\n",
    "        # Create a new Google Doc with the title\n",
    "        doc = drive_service.files().create(\n",
    "            body={\n",
    "                'name': title,\n",
    "                'mimeType': 'application/vnd.google-apps.document'\n",
    "            }\n",
    "        ).execute()\n",
    "\n",
    "        # Retrieve the document ID\n",
    "        document_id = doc['id']\n",
    "\n",
    "        # Insert the content into the Google Doc\n",
    "        requests = [\n",
    "            {\n",
    "                'insertText': {\n",
    "                    'location': {\n",
    "                        'index': 1,\n",
    "                    },\n",
    "                    'text': content\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        docs_service.documents().batchUpdate(\n",
    "            documentId=document_id,\n",
    "            body={'requests': requests}\n",
    "        ).execute()\n",
    "\n",
    "        logging.info(f\"Created and populated Google Doc with ID: {document_id}\")\n",
    "        return document_id\n",
    "\n",
    "    except HttpError as error:\n",
    "        logging.error(f\"An error occurred: {error}\")\n",
    "\n",
    "\n",
    "# Function to set Google Doc permissions to public edit\n",
    "def set_public_edit(document_id):\n",
    "    try:\n",
    "        # Create a permission to make Google Doc publicly editable\n",
    "        permission = {\n",
    "            'type': 'anyone',\n",
    "            'role': 'writer'\n",
    "        }\n",
    "        \n",
    "        drive_service.permissions().create(\n",
    "            fileId=document_id,\n",
    "            body=permission\n",
    "        ).execute()\n",
    "        \n",
    "        logging.info(f\"Document with ID {document_id} is now publicly editable.\")\n",
    "        \n",
    "    except HttpError as error:\n",
    "        logging.error(f\"An error occurred: {error}\")\n",
    "\n",
    "# Function to extract title and meta description from text\n",
    "def extract_title_and_meta(text):\n",
    "    title_tag_match = re.search(r'Title tag: (.+)', text)\n",
    "    meta_desc_match = re.search(r'Meta description: (.+)', text)\n",
    "    \n",
    "    title_tag = title_tag_match.group(1) if title_tag_match else \"Not found\"\n",
    "    meta_description = meta_desc_match.group(1) if meta_desc_match else \"Not found\"\n",
    "\n",
    "    title_length = len(title_tag)\n",
    "    meta_length = len(meta_description)\n",
    "\n",
    "    return title_tag, meta_description, title_length, meta_length\n",
    "\n",
    "# Function to log various scores like readability, keyword density, etc.\n",
    "def log_scores(text, title_tag, meta_description, title_length, meta_length, text_type=\"original\"):\n",
    "    mistakes = spellcheck_uk(text)\n",
    "    main_keyword = identify_important_keywords(text).strip('.')\n",
    "    density = keyword_density(text, main_keyword)\n",
    "    word_count = len(text.split())\n",
    "\n",
    "    logging.info(f\"Scores for {text_type} text:\")\n",
    "    logging.info(f\"Title Tag: {title_tag}\")\n",
    "    logging.info(f\"Title Length: {title_length} characters\")\n",
    "    logging.info(f\"Meta Description: {meta_description}\")\n",
    "    logging.info(f\"Meta Description Length: {meta_length} characters\")\n",
    "    logging.info(f\"Spellcheck Mistakes: {mistakes}\")\n",
    "    logging.info(f\"Main Keyword: {main_keyword}\")\n",
    "    logging.info(f\"Keyword Density: {density}%\")\n",
    "    logging.info(f\"Total Word Count: {word_count}\")\n",
    "\n",
    "# Function to append text to an existing Google Doc\n",
    "def append_text_to_google_doc(document_id, text):\n",
    "    requests = [\n",
    "        {\n",
    "            'insertText': {\n",
    "                'text': text,\n",
    "                'location': {\n",
    "                    'index': 1,  # Appending at the end of the document\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "    docs_service.documents().batchUpdate(documentId=document_id, body={'requests': requests}).execute()\n",
    "\n",
    "def rephrase_sentence(sentence, keyword):\n",
    "    # Use GPT to rephrase the sentence to exclude the keyword\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant specialized in rephrasing text.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Rephrase the following sentence to exclude the keyword '{keyword}':\\n{sentence}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "\n",
    "def adjust_keyword_density(text, keyword, min_density, max_density):\n",
    "    total_words = len(text.split())\n",
    "    keyword_occurrences = text.lower().count(keyword.lower())\n",
    "    current_density = (keyword_occurrences / total_words) * 100\n",
    "\n",
    "    if current_density < min_density:\n",
    "        # Insert the keyword to increase density\n",
    "        words = text.split()\n",
    "        while current_density < min_density:\n",
    "            position = random.randint(0, len(words) - 1)\n",
    "            words.insert(position, keyword)\n",
    "            total_words += 1\n",
    "            keyword_occurrences += 1\n",
    "            current_density = (keyword_occurrences / total_words) * 100\n",
    "        text = ' '.join(words)\n",
    "\n",
    "    elif current_density > max_density:\n",
    "        # Remove the keyword to decrease density\n",
    "        words = text.split()\n",
    "        keyword_lower = keyword.lower()\n",
    "        while current_density > max_density:\n",
    "            for i, word in enumerate(words):\n",
    "                if word.lower() == keyword_lower:\n",
    "                    del words[i]\n",
    "                    total_words -= 1\n",
    "                    keyword_occurrences -= 1\n",
    "                    current_density = (keyword_occurrences / total_words) * 100\n",
    "                    break\n",
    "        text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to generate the text that includes the improved scores\n",
    "def generate_score_text(text, title_length, meta_length):\n",
    "    title_tag_match = re.search(r'Title tag: (.+)', text)\n",
    "    meta_desc_match = re.search(r'Meta description: (.+)', text)\n",
    "    \n",
    "    title_tag = title_tag_match.group(1) if title_tag_match else \"Not found\"\n",
    "    meta_description = meta_desc_match.group(1) if meta_desc_match else \"Not found\"\n",
    "    mistakes = spellcheck_uk(text)\n",
    "    main_keyword = identify_important_keywords(text).strip('.')\n",
    "    density = keyword_density(text, main_keyword)\n",
    "\n",
    "    word_count = len(text.split())\n",
    "    title_length = len(title_tag)\n",
    "    meta_length = len(meta_description)\n",
    "\n",
    "    score_text = (\n",
    "        f\"Scores for Improved Text:\\n\"\n",
    "        f\"Title Length: {title_length} characters\\n\"\n",
    "        f\"Meta Description Length: {meta_length} characters\\n\"\n",
    "        f\"Spellcheck Mistakes: {mistakes}\\n\"\n",
    "        f\"Main Keyword: {main_keyword}\\n\"\n",
    "        f\"Keyword Density: {density}%\\n\"\n",
    "        f\"Total Word Count: {word_count}\\n\"\n",
    "     )\n",
    "    return score_text\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    document_id = \"1jReaLd1cs9DDVUhY484PnG48vWGH2vSReq-GlJ3x-fo\"\n",
    "    text = read_google_doc(document_id)\n",
    "\n",
    "    # Identify the position where 'H1' is present\n",
    "    h1_position = text.find('H1')\n",
    "\n",
    "    # Start from 'H1' and continue improving text from there\n",
    "    text_starting_from_h1 = text[h1_position:]\n",
    "\n",
    "    # Improve text starting from H1\n",
    "    improved_text_starting_from_h1 = rephrase_to_professional_uk(text_starting_from_h1)\n",
    "    \n",
    "    # Replace only the part of the text starting from H1 with the improved text\n",
    "    improved_text = text[:h1_position] + improved_text_starting_from_h1\n",
    "\n",
    "    # Extract title and meta information\n",
    "    title_tag, meta_description, title_length, meta_length = extract_title_and_meta(improved_text)\n",
    "    \n",
    "    # Identify the main keyword in the improved text starting from H1\n",
    "    main_keyword = identify_important_keywords(text_starting_from_h1)\n",
    "\n",
    "    # Adjust keyword density of improved_text_starting_from_h1\n",
    "    adjusted_improved_text_starting_from_h1 = adjust_keyword_density(improved_text_starting_from_h1, main_keyword, min_density=1.0, max_density=2.0)\n",
    "    \n",
    "    # Replace only the part of the text starting from H1 with the adjusted improved text\n",
    "    adjusted_improved_text = text[:h1_position] + adjusted_improved_text_starting_from_h1\n",
    "\n",
    "    # Update the existing Google Doc with the adjusted improved text\n",
    "    update_google_doc(document_id, adjusted_improved_text)\n",
    "\n",
    "    # Extract title and meta information from the adjusted text\n",
    "    title_tag, meta_description, title_length, meta_length = extract_title_and_meta(adjusted_improved_text)\n",
    "    \n",
    "    # Create a new Google Doc with the adjusted improved text\n",
    "    new_document_id = create_google_doc(\"Improved Document\", adjusted_improved_text)\n",
    "    set_public_edit(new_document_id)\n",
    "    \n",
    "    # Log and print scores for both original and improved texts\n",
    "    log_scores(text, title_tag, meta_description, title_length, meta_length, \"original\")\n",
    "    log_scores(adjusted_improved_text, title_tag, meta_description, title_length, meta_length, \"improved\")\n",
    "    \n",
    "    # Generate and append score text to the new Google Doc\n",
    "    score_text = generate_score_text(adjusted_improved_text, title_length, meta_length)\n",
    "    append_text_to_google_doc(new_document_id, score_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
